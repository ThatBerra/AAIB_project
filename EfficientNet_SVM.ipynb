{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoHZ78wY7FC0"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9aSW77j7OAe"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10.1  -q gwpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtmEMiFx7Uaq"
   },
   "outputs": [],
   "source": [
    "!pip uninstall matplotlib\n",
    "!pip install matplotlib==3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltIU-wtn7YyF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "from google.colab import drive\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import albumentations as A\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array,array_to_img\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.callbacks import *\n",
    "import random\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "from skimage.measure import label as label_fn\n",
    "import cv2\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(tf.__version__)\n",
    "import albumentations as A\n",
    "from itertools import combinations\n",
    "import json\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from skimage import filters\n",
    "import scipy\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM-24HHW79aS"
   },
   "source": [
    "# Mount the My Drive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtm7cvGdbuMm"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB-hx4_q-C1O"
   },
   "source": [
    "# Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZesDeAvabgmY"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/AAIB\n",
    "PATH = '/content/drive/MyDrive/tuberculosis-pneumonia-classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vgluqkda_cWK"
   },
   "outputs": [],
   "source": [
    "SEED = 4224\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "labels_path = 'data/labels_train_clean.csv'\n",
    "all_data_no_duplicates_path = 'data/train_all_no_duplicates'\n",
    "clean_data_path = 'data/train_clean/'\n",
    "noisy_data_path = 'data/train_noisy/'\n",
    "\n",
    "train_percentage = 0.8\n",
    "validation_percentage = 0.15\n",
    "test_percentage = 0.2\n",
    "img_size = (224,224)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5F4KwVvkfqO"
   },
   "source": [
    "#Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpkXx4uSkiFb"
   },
   "outputs": [],
   "source": [
    "class CustomGenerator(tf.keras.utils.Sequence):\n",
    "  \"\"\"\n",
    "    CustomGenerator inheriting from tf.keras.utils.Sequence.\n",
    "\n",
    "    We have to implement 3 main methods:\n",
    "      - __init__: save dataset params like directory, filenames, etc.\n",
    "      - __len__: return the total number of samples in the dataset (number of batches)\n",
    "      - __getitem__: return a single batch of paired images masks\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, \n",
    "               dataframe, # dataframe of the dataset  \n",
    "               base_path,\n",
    "               preprocessing_function=None, # Preprocessing function (e.g., the one used for transfer learning)\n",
    "               batch_size=16, # Batch size\n",
    "               out_shape = (100,100),\n",
    "               shuffle=False,\n",
    "               categorical = True,\n",
    "               augment = False,\n",
    "               seed = SEED,\n",
    "               flow_from_directory = True,\n",
    "               preprocess_input = False):\n",
    "    \n",
    "    # Get all filenames\n",
    "    if isinstance(base_path, Tuple):\n",
    "      self.filenames = []\n",
    "      for p in base_path:\n",
    "\n",
    "        paths = self.folderToPaths(p, full_path = False)\n",
    "\n",
    "        for pa in paths:\n",
    "          if pa in set(dataframe.file):\n",
    "            self.filenames.append(os.path.join(p, pa))\n",
    "\n",
    "\n",
    "    else:\n",
    "        self.filenames = [os.path.join(base_path, img_path) for img_path in list(dataframe.file)]\n",
    "\n",
    "    self.labels = tfk.utils.to_categorical(list(dataframe.label)) if categorical else list(dataframe.label)\n",
    "\n",
    "    # Set indices list in [0, len(subset_filenames)]\n",
    "    self.indices = np.arange(len(self.filenames))\n",
    "\n",
    "    # Save dataset parameters as class attributes\n",
    "    self.base_path = base_path\n",
    "    self.preprocessing_function = preprocessing_function\n",
    "    self.out_shape = out_shape\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = shuffle\n",
    "    self.augment = augment\n",
    "    self.seed = seed\n",
    "    self.flow_from_directory =flow_from_directory\n",
    "    self.data_augmentation = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit = 0.05, contrast_limit=0.05, p=0.5),\n",
    "    A.ShiftScaleRotate(p = 0.8, rotate_limit = 20, scale_limit = 0.3, border_mode =  cv2.BORDER_CONSTANT, value = 0),\n",
    "    A.CLAHE(p=0.2)\n",
    "    ])\n",
    "    self.preprocess_input = preprocess_input\n",
    "    \n",
    "    if preprocess_input:\n",
    "      self.noisyClahe = cv2.createCLAHE(clipLimit = 300, tileGridSize = (50, 50))\n",
    "      self.blurredClahe = cv2.createCLAHE(clipLimit = 1.8, tileGridSize = (4, 4))\n",
    "\n",
    "    if not self.flow_from_directory:\n",
    "      self.images = self.load_all_imgs()\n",
    "\n",
    "  def augmentation(self, images):\n",
    "    return self.data_augmentation(image = images)\n",
    "\n",
    "\n",
    "  def __filterNoisyOnClahe(self, image):\n",
    "    im1 = cv2.resize(image, (400, 400))\n",
    "    im1 = scipy.ndimage.gaussian_laplace(im1, sigma = 6)\n",
    "    im1 = self.noisyClahe.apply(im1)\n",
    "    var1 = np.var(im1)\n",
    "    if var1 > 800:\n",
    "      image= cv2.medianBlur(image, ksize=5)\n",
    "      return scipy.ndimage.uniform_filter(image, size=3)\n",
    "    else:\n",
    "      return image\n",
    "\n",
    "\n",
    "  def __sharpenImage(self, image):\n",
    "    sharpen_kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "\n",
    "    sharpened = cv2.filter2D(image, -1, sharpen_kernel)\n",
    "\n",
    "    return sharpened\n",
    "\n",
    "  def __invert_image(self, img):\n",
    "\n",
    "    otsu_thresh = filters.threshold_otsu(img)\n",
    "    masked_image = (img > otsu_thresh) * 1.0\n",
    "    valsROI1, _ = np.histogram(mask[220:244, 220:244], bins=2, range=(0, 1))\n",
    "    valsROI2, _ = np.histogram(mask[0:25, 0:25], bins=2, range=(0, 1))\n",
    "    valsROI3, _ = np.histogram(mask[0:25, 220:244], bins=2, range=(0, 1))\n",
    "    valsROI4, _ = np.histogram(mask[220:244, 0:25], bins=2, range=(0, 1))\n",
    "    \n",
    "    valsTot = valsROI1 + valsROI2 + valsROI3 + valsROI4\n",
    "    \n",
    "    labels = label_fn(masked_image)\n",
    "\n",
    "    if len(np.unique(labels)) < 100:\n",
    "        if valsTot[0] > valsTot[1]:\n",
    "            return img\n",
    "        else:\n",
    "            return 255 - img\n",
    "    return img\n",
    "\n",
    "\n",
    "  def __filterBlurred(self, image):\n",
    "    minThresh = 2\n",
    "    im1 = cv2.resize(image, (400, 400))\n",
    "    im1 = scipy.ndimage.gaussian_laplace(im1, sigma = 2)\n",
    "    im1 = self.blurredClahe.apply(im1)\n",
    "    _, im1 = cv2.threshold(im1, minThresh, 255, cv2.THRESH_BINARY)\n",
    "    var1 = np.var(im1)\n",
    "    if var1 < 5:\n",
    "        return self.__sharpenImage(image)\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "  def __filterUnderexposed(self, image):\n",
    "    im1 = cv2.resize(image, (400, 400))\n",
    "    mean1 = np.mean(im1)\n",
    "    if mean1 < 71:\n",
    "      clahe = cv2.createCLAHE(clipLimit = 2, tileGridSize = (2, 2))\n",
    "      image = clahe.apply(image)\n",
    "      return image\n",
    "    else:\n",
    "      return image\n",
    "\n",
    "\n",
    "  def preprocess(self, image):\n",
    "\n",
    "    image = self.__invert_image(image)\n",
    "    image = self.__filterUnderexposed(image)\n",
    "    image = self.__filterNoisyOnClahe(image)\n",
    "    image = self.__filterBlurred(image)\n",
    "   \n",
    "\n",
    "    return image\n",
    "\n",
    "  def __len__(self):\n",
    "    # Return the length of the dataset (number of batches)\n",
    "    # that is given by #images // batch_size\n",
    "    return len(self.filenames) // self.batch_size\n",
    "\n",
    "  def on_epoch_begin(self):\n",
    "    # Shuffle indices after each epoch\n",
    "    if self.shuffle == True:\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "  def load_all_imgs(self):\n",
    "      images = []\n",
    "      for f in self.filenames:\n",
    "        image = cv2.imread(f, 0)\n",
    "        image = cv2.resize(image, (self.out_shape))\n",
    "        if self.preprocess_input:\n",
    "          image = self.preprocess(image)\n",
    "        images.append(image)\n",
    "\n",
    "      return np.array(images)\n",
    "\n",
    "  def get_image_and_label(self, index):\n",
    "\n",
    "    if not self.flow_from_directory:\n",
    "      image = self.images[index]\n",
    "      if self.augment:\n",
    "        image = self.augmentation(image)\n",
    "      image = np.squeeze(image)\n",
    "      image = np.stack([image, image, image], axis = -1)\n",
    "      curr_label = self.labels[index]\n",
    "    else:\n",
    "      curr_filename = self.filenames[index] # Get filename at index\n",
    "      curr_label = self.labels[index]\n",
    "      image = cv2.imread(curr_filename, 0)\n",
    "      image = cv2.resize(image, (self.out_shape))\n",
    "      if self.preprocess_input:\n",
    "        image = self.preprocess(image)\n",
    "\n",
    "      if self.augment:\n",
    "        image = self.augmentation(image)['image']\n",
    "\n",
    "      image = np.stack([image, image, image], axis = -1)\n",
    "\n",
    "\n",
    "    return image, curr_label\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # In this function we generate a batch (of size self.batch_size) of images and corresponding masks\n",
    "    \n",
    "    # Get 'self.batch_size' indices\n",
    "    current_indices = self.indices[index*self.batch_size:(index*self.batch_size)+self.batch_size]\n",
    "\n",
    "    \"\"\"if len(current_indices) == 0:\n",
    "      current_indices = self.indices[len(self.indices)-self.batch_size:len(self.indices)]\"\"\"\n",
    "\n",
    "    # Init lists that will contain images and masks\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "\n",
    "    # Cycle over the indices\n",
    "    for idx in current_indices:\n",
    "      # Get single image/mask at index 'idx'\n",
    "      image, label = self.get_image_and_label(idx)\n",
    "\n",
    "      # Apply the preprocessing function\n",
    "      if self.preprocessing_function is not None:\n",
    "        image = self.preprocessing_function(image)\n",
    "\n",
    "      # Append both image and mask (with added batch dimension) to the corresponding batch lists\n",
    "      batch_images.append(np.expand_dims(image, 0))\n",
    "      batch_labels.append(label)\n",
    "     \n",
    "    # Finally, obtain a final batch by concatenating all the images over the batch dimension\n",
    "    batch_images = np.concatenate(batch_images, axis=0)\n",
    "    batch_labels = np.array(batch_labels)\n",
    "\n",
    "    return batch_images, batch_labels\n",
    "\n",
    "\n",
    "  def folderToPaths(\n",
    "        self,\n",
    "        full_img_dir,\n",
    "        full_path = True\n",
    "):\n",
    "\n",
    "    x_paths_list = []\n",
    "\n",
    "    full_img_dir = full_img_dir\n",
    "\n",
    "    for full in os.listdir(full_img_dir):\n",
    "         if full_path:\n",
    "            x_paths_list.append(os.path.join(full_img_dir, full))\n",
    "         else:\n",
    "          x_paths_list.append(full)\n",
    "    \n",
    "    x_paths_list.sort()\n",
    "    return x_paths_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lrNseD9kmY_"
   },
   "source": [
    "#Cyclical LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piTn7ha6kxiZ"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSGtqSaGk8pN"
   },
   "source": [
    "#Data loading (all data, no duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcjXe05uk8DB"
   },
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "  if x == 'N':\n",
    "    return 0\n",
    "  elif x == 'P':\n",
    "    return 1\n",
    "  else:\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Gwff9LClGIp"
   },
   "outputs": [],
   "source": [
    "def folderToPaths(\n",
    "        full_img_dir,\n",
    "):\n",
    "\n",
    "    x_paths_list = []\n",
    "\n",
    "    full_img_dir = full_img_dir\n",
    "\n",
    "    for full in os.listdir(full_img_dir):\n",
    "         x_paths_list.append(os.path.join(full_img_dir, full))\n",
    "    \n",
    "    x_paths_list.sort()\n",
    "    return x_paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHUtXX4mlLcU"
   },
   "outputs": [],
   "source": [
    "labelsDF = pd.read_csv(labels_path)\n",
    "display(labelsDF.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8qbtiy2lTzN"
   },
   "outputs": [],
   "source": [
    "labelsDF.label = labelsDF.label.apply(lambda x: encode(x))\n",
    "display(labelsDF.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1ffgaK6lWR4"
   },
   "outputs": [],
   "source": [
    "len(set(labelsDF.file)) # 1 acquisition per patienty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIx_fCgglYja"
   },
   "outputs": [],
   "source": [
    "all_data_no_duplicates_path_list = folderToPaths(full_img_dir = all_data_no_duplicates_path)\n",
    "\"\"\"clean_data_path_list = folderToPaths(full_img_dir = clean_data_path)\n",
    "noisy_data_path_lust = folderToPaths(full_img_dir = noisy_data_path)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpHf0RAxla4F"
   },
   "outputs": [],
   "source": [
    "len(all_data_no_duplicates_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4IWln0klctE"
   },
   "outputs": [],
   "source": [
    "train_val, test = train_test_split(labelsDF, test_size = test_percentage, shuffle = True, stratify = labelsDF.label, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiI38CAtle-n"
   },
   "outputs": [],
   "source": [
    "test_gen = CustomGenerator(dataframe = test, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = True, augment = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6qBzxsllh9b"
   },
   "outputs": [],
   "source": [
    "dataset_labels = np.array(list(set(labelsDF.label)), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUiIJRTnlkBa"
   },
   "outputs": [],
   "source": [
    "iterator = iter(test_gen)\n",
    "images, labels = next(iterator)\n",
    "fig, axis = plt.subplots(4, 4, figsize = (20, 20))\n",
    "\n",
    "axis = axis.flatten()\n",
    "\n",
    "for i in range(images.shape[0]):\n",
    "  axis[i].imshow(images[i].squeeze(), cmap='gray')\n",
    "  axis[i].set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0tcss01lmHT"
   },
   "source": [
    "# SVM\n",
    "code adapted from https://github.com/AryaAftab/svm-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOsSAyfuMNFH"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'ipykernel' in sys.modules:\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "\n",
    "class ShowProgress(callbacks.Callback):\n",
    "    def __init__(self, epochs, step_show=1, metric=\"accuracy\"):\n",
    "        super(ShowProgress, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.step_show = step_show\n",
    "        self.metric = metric\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.pbar = tqdm(range(self.epochs))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.step_show == 0:\n",
    "\n",
    "            self.pbar.set_description(f\"\"\"Epoch : {epoch + 1} / {self.epochs}, \n",
    "            Train {self.metric} : {round(logs[self.metric], 4)}, \n",
    "            Valid {self.metric} : {round(logs['val_' + self.metric], 4)}\"\"\")\n",
    "\n",
    "            self.pbar.update(self.step_show)\n",
    "\n",
    "            \n",
    "class BestModelWeights(callbacks.Callback):\n",
    "    def __init__(self, metric=\"val_accuracy\", metric_type=\"max\"):\n",
    "        super(BestModelWeights, self).__init__()\n",
    "        self.metric = metric\n",
    "        self.metric_type = metric_type\n",
    "        if self.metric_type not in [\"min\", \"max\"]:\n",
    "                raise NameError('metric_type must be min or max')\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if self.metric_type == \"min\":\n",
    "            self.best_metric = math.inf\n",
    "        else:\n",
    "            self.best_metric = -math.inf\n",
    "        self.best_epoch = 0\n",
    "        self.model_best_weights = None\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.metric_type == \"min\":\n",
    "            if self.best_metric >= logs[self.metric]:\n",
    "                self.model_best_weights = self.model.get_weights()\n",
    "                self.best_metric = logs[self.metric]\n",
    "                self.best_epoch = epoch\n",
    "        else:\n",
    "            if self.best_metric <= logs[self.metric]:\n",
    "                self.model_best_weights = self.model.get_weights()\n",
    "                self.best_metric = logs[self.metric]\n",
    "                self.best_epoch = epoch\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.model.set_weights(self.model_best_weights)\n",
    "        print(f\"\\nBest weights is set, Best Epoch was : {self.best_epoch+1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Hy8YofFMHos"
   },
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "#classes\n",
    "\n",
    "class LinearSVC(layers.Layer):\n",
    "    def __init__(self, num_classes=2, **kwargs):\n",
    "        super(LinearSVC, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "        self.reg_loss = lambda weight : 0.5 * tf.reduce_sum(tf.square(weight))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.num_classes),\n",
    "            initializer=tf.random_normal_initializer(stddev=0.1),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.num_classes,), initializer=tf.constant_initializer(value=0.1),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        loss = self.reg_loss(self.w)\n",
    "        self.add_loss(loss)\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(LinearSVC, self).get_config()\n",
    "        config.update({\"num_classes\": self.num_classes})\n",
    "        return config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SVMTrainer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_class,\n",
    "        C=1.0,\n",
    "        bone=None,\n",
    "        name=\"SVMTrainer\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(SVMTrainer, self).__init__(name=name, **kwargs)\n",
    "    \n",
    "        self.num_class = num_class\n",
    "\n",
    "        if bone is None:\n",
    "            self.bone = lambda x: tf.identity(x)\n",
    "        else:\n",
    "            self.bone = bone\n",
    "\n",
    "        self.linear_svc = LinearSVC(self.num_class)\n",
    "        self.C = C\n",
    "        \n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "    \n",
    "    \n",
    "    def svc_loss(self, y_true, y_pred, sample_weight, reg_loss):\n",
    "        \n",
    "        loss = tf.keras.losses.categorical_hinge(y_true ,y_pred)\n",
    "        if sample_weight is not None:\n",
    "            loss = sample_weight * loss\n",
    "        \n",
    "        return reg_loss + self.C * loss\n",
    "    \n",
    "    \n",
    "    def compile(self, **kwargs):\n",
    "        super(SVMTrainer, self).compile(**kwargs)\n",
    "        self.compiled_loss = None\n",
    "    \n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        x = self.bone(x)\n",
    "        x = self.linear_svc(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        if len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            sample_weight = None\n",
    "            x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.svc_loss(\n",
    "                y,\n",
    "                y_pred,\n",
    "                sample_weight=sample_weight,\n",
    "                reg_loss=self.losses,\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        if self.num_class == 2:\n",
    "            y = y[..., 1]\n",
    "            y_pred = tf.sigmoid(y_pred[..., 1])\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    \n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        if len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            sample_weight = None\n",
    "            x, y = data\n",
    "        # Compute predictions\n",
    "        y_pred = self(x, training=False)\n",
    "        # Updates the metrics tracking the loss\n",
    "        loss = self.svc_loss(\n",
    "                y,\n",
    "                y_pred,\n",
    "                sample_weight=sample_weight,\n",
    "                reg_loss=self.losses,\n",
    "        )\n",
    "        # Update the metrics.\n",
    "        if self.num_class == 2:\n",
    "            y = y[..., 1]\n",
    "            y_pred = tf.sigmoid(y_pred[..., 1])\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch or at the start of `evaluate()`.\n",
    "        return [self.loss_tracker] + self.compiled_metrics.metrics\n",
    "\n",
    "\n",
    "    def save(self, model_path=None, input_shape=None):\n",
    "        input_shape = [1] + input_shape \n",
    "        dumy_input = np.random.rand(*input_shape)\n",
    "\n",
    "\n",
    "        dumy_body_output = self.bone(dumy_input)\n",
    "        dumy_head_output = self.linear_svc(dumy_body_output)\n",
    "\n",
    "\n",
    "        head_part = layers.Dense(units=dumy_head_output.shape[-1], activation=\"sigmoid\")\n",
    "        _ = head_part(dumy_body_output)\n",
    "        head_part.set_weights(self.linear_svc.get_weights())\n",
    "\n",
    "\n",
    "        if isinstance(self.bone, types.FunctionType):\n",
    "            body_part = layers.Lambda(lambda x: self.bone(x))\n",
    "        else:\n",
    "            body_part = self.bone\n",
    "\n",
    "\n",
    "        input_shape.pop(0)\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = body_part(inputs)\n",
    "        x = head_part(x)\n",
    "\n",
    "\n",
    "        model = tf.keras.models.Model(inputs, x)\n",
    "        model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning model no k fold"
   ],
   "metadata": {
    "id": "N6QWWb3zJHDF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train, val = train_test_split(train_val, test_size = validation_percentage, shuffle = True, stratify = train_val.label, random_state=SEED)\n",
    "train_gen = CustomGenerator(dataframe = train, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = True, augment = False)\n",
    "valid_gen = CustomGenerator(dataframe = val, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = True, augment = False)\n",
    "test_gen = CustomGenerator(dataframe = test, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = True, augment = False)"
   ],
   "metadata": {
    "id": "W_-OPsKTJS2T"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_supernet():\n",
    "  supernet1 = tf.keras.applications.efficientnet_v2.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "\n",
    "  count = 1\n",
    "  print(len(supernet1.layers))\n",
    "  for layer in supernet1.layers:\n",
    "      if count < 80:\n",
    "          layer.trainable = False\n",
    "      else:\n",
    "          layer.trainable = True\n",
    "      count = count + 1\n",
    "  \n",
    "  return supernet1"
   ],
   "metadata": {
    "id": "J1MmYvsvJMTy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define metrics\n",
    "METRICS = [tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "]\n",
    "\n",
    "  # Define Bone, if you want linear svm, you can pass None to SVMTrainer as bone\n",
    "Bone = tf.keras.models.Sequential([\n",
    "      tfk.Input((224,224,3)),\n",
    "      get_supernet(),\n",
    "      tf.keras.layers.GlobalAveragePooling2D()\n",
    "  ])\n",
    "\n",
    "training_samples = int(len(train_gen)*batch_size)\n",
    "step_size = 6*training_samples // batch_size\n",
    "\n",
    "clr = CyclicLR(\n",
    "      mode='triangular',\n",
    "      base_lr=1e-5, \n",
    "      max_lr=1e-4,\n",
    "      step_size= step_size\n",
    "      )\n",
    "\n",
    "\n",
    "svm_model = SVMTrainer(num_class=3, bone=Bone, C = 10)\n",
    "svm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                    metrics=METRICS)\n",
    " \n",
    "epochs = 100\n",
    "\n",
    "  # Train\n",
    "history = svm_model.fit(train_gen,\n",
    "                          epochs=epochs,  validation_data = valid_gen,\n",
    "                          callbacks = [tfk.callbacks.EarlyStopping(monitor= 'val_accuracy', mode='max', patience=15, restore_best_weights=True), clr], workers =8, use_multiprocessing = True\n",
    "                          )\n",
    "\n"
   ],
   "metadata": {
    "id": "FUSDsig_Jf72"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history.history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history.history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Category Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history.history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "9T4GIVDOKvs1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#prediction on test set\n",
    "predictions = svm_model.predict(test_gen)\n",
    "\n",
    "y = np.argmax(tfk.utils.to_categorical(list(test.label))[:-2], axis = -1)\n",
    "pred = np.argmax(predictions, axis=-1)\n",
    "target_names = ['N', 'P', 'T']\n",
    "cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(y, pred)\n",
    "precision = precision_score(y, pred, average='macro')\n",
    "recall = recall_score(y, pred, average='macro')\n",
    "f1 = f1_score(y, pred, average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "hm = sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()\n",
    "\n",
    "hm.get_figure().savefig(\"data_for_report/heatmap_eff_svm_all_data.pdf\")\n",
    "\n",
    "# Model saving\n",
    "model_directory = './'\n",
    "filename = 'svm_all_data'\n",
    "filename_chosen = os.path.join(model_directory, filename)\n",
    "svm_model.save(filename_chosen + '.h5', input_shape=[224, 224, 3])\n",
    "\n",
    "\n",
    "with open('data_for_report/train_all_data_eff_svm_history.json' , 'w') as fp:\n",
    "    json.dump(history.history, fp)\n",
    "\n",
    "with open('data_for_report/test_all_data_eff_svm_report.json' , 'w') as fp:\n",
    "    json.dump(classification_report(y, pred, target_names=target_names, digits=4, output_dict = True), fp)"
   ],
   "metadata": {
    "id": "Cs533FN7Kuhw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SC6z1y1eltHC"
   },
   "source": [
    "#Learning model k fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZomQIyiQlzIK"
   },
   "outputs": [],
   "source": [
    "def get_supernet():\n",
    "  supernet1 = tf.keras.applications.efficientnet_v2.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "\n",
    "  count = 1\n",
    "  print(len(supernet1.layers))\n",
    "  for layer in supernet1.layers:\n",
    "      if count < 80:\n",
    "          layer.trainable = False\n",
    "      else:\n",
    "          layer.trainable = True\n",
    "      count = count + 1\n",
    "  \n",
    "  return supernet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwefduPaNAm2"
   },
   "outputs": [],
   "source": [
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "val_acc_per_fold = []\n",
    "val_loss_per_fold = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = SEED)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(skf.split(train_val.file.to_numpy(), train_val.label.to_numpy())): \n",
    "  trainDF = train_val.filter(items= train_index, axis=0)\n",
    "  validDF = train_val.filter(items= valid_index, axis=0)\n",
    "\n",
    "  train_gen = CustomGenerator(dataframe = trainDF, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = True, augment = False)\n",
    "  valid_gen = CustomGenerator(dataframe = validDF, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = True, augment = False)\n",
    "  # Define metrics\n",
    "  METRICS = [tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "  ]\n",
    "\n",
    "  # Define Bone, if you want linear svm, you can pass None to SVMTrainer as bone\n",
    "  Bone = tf.keras.models.Sequential([\n",
    "      tfk.Input((224,224,3)),\n",
    "      get_supernet(),\n",
    "      tf.keras.layers.GlobalAveragePooling2D()\n",
    "  ])\n",
    "\n",
    "  training_samples = int(len(train_gen)*batch_size)\n",
    "  step_size = 6*training_samples // batch_size\n",
    "\n",
    "  clr = CyclicLR(\n",
    "      mode='triangular',\n",
    "      base_lr=1e-5, \n",
    "      max_lr=1e-4,\n",
    "      step_size= step_size\n",
    "      )\n",
    "\n",
    "\n",
    "  svm_model = SVMTrainer(num_class=3, bone=Bone, C = 10)\n",
    "  svm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                    metrics=METRICS)\n",
    " \n",
    "  epochs = 100\n",
    "\n",
    "  # Train\n",
    "  history = svm_model.fit(train_gen,\n",
    "                          epochs=epochs,  validation_data = valid_gen,\n",
    "                          callbacks = [tfk.callbacks.EarlyStopping(monitor= 'val_accuracy', mode='max', patience=15, restore_best_weights=True), clr], workers =8, use_multiprocessing = True\n",
    "                          )\n",
    "  \n",
    "  val_acc_per_fold.append(history.history[\"val_accuracy\"] * 100)\n",
    "  val_loss_per_fold.append(history.history[\"val_loss\"])\n",
    "  acc_per_fold.append(history.history[\"accuracy\"] * 100)\n",
    "  loss_per_fold.append(history.history[\"loss\"])\n",
    "\n",
    "  # Plot the training\n",
    "  plt.figure(figsize=(20,5))\n",
    "  plt.plot(history.history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "  plt.plot(history.history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "  plt.legend(loc='upper left')\n",
    "  plt.title('Category Crossentropy')\n",
    "  plt.grid(alpha=.3)\n",
    "\n",
    "  plt.figure(figsize=(20,5))\n",
    "  plt.plot(history.history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "  plt.plot(history.history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "  plt.legend(loc='upper left')\n",
    "  plt.title('Accuracy')\n",
    "  plt.grid(alpha=.3)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "  #prediction on test set\n",
    "  predictions = svm_model.predict(test_gen)\n",
    "\n",
    "  y = np.argmax(tfk.utils.to_categorical(list(test.label))[:-2], axis = -1)\n",
    "  pred = np.argmax(predictions, axis=-1)\n",
    "  target_names = ['N', 'P', 'T']\n",
    "  cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "  # Compute the classification metrics\n",
    "  accuracy = accuracy_score(y, pred)\n",
    "  precision = precision_score(y, pred, average='macro')\n",
    "  recall = recall_score(y, pred, average='macro')\n",
    "  f1 = f1_score(y, pred, average='macro')\n",
    "  print('Accuracy:',accuracy.round(4))\n",
    "  print('Precision:',precision.round(4))\n",
    "  print('Recall:',recall.round(4))\n",
    "  print('F1:',f1.round(4))\n",
    "  print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "  # Plot the confusion matrix\n",
    "  plt.figure(figsize=(10,10))\n",
    "  sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "  plt.xlabel('True labels')\n",
    "  plt.ylabel('Predicted labels')\n",
    "  plt.show()\n",
    "\n",
    "  # Model saving\n",
    "  model_directory = './'\n",
    "  filename = 'svm_kFold_' + str(i+1)\n",
    "  filename_chosen = os.path.join(model_directory, filename)\n",
    "  svm_model.save(filename_chosen + '.h5', input_shape=[224, 224, 3])"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "best_train_acc_fold = []\n",
    "best_train_loss_fold = []\n",
    "best_val_acc_fold = []\n",
    "best_val_loss_fold = []\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  best_acc_train_index = np.argmax(acc_per_fold[i])\n",
    "  best_acc_train = acc_per_fold[i][best_acc_train_index]\n",
    "  best_loss_train = loss_per_fold[i][best_acc_train_index]\n",
    "\n",
    "  best_acc_val_index = np.argmax(val_acc_per_fold[i])\n",
    "  best_acc_val = acc_per_fold[i][best_acc_val_index]\n",
    "  best_loss_val = loss_per_fold[i][best_acc_val_index]\n",
    "\n",
    "  best_train_acc_fold.append(best_acc_train)\n",
    "  best_train_loss_fold.append(best_loss_train)\n",
    "  best_val_acc_fold.append(best_acc_val)\n",
    "  best_val_loss_fold.append(best_loss_val)"
   ],
   "metadata": {
    "id": "HGo_BFhbzppo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FL7mx8eBUM8H"
   },
   "outputs": [],
   "source": [
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - TLoss: {best_train_loss_fold[i]} - TAccuracy: {best_train_acc_fold[i]}%')\n",
    "  print(f'> Fold {i+1} - VLoss: {best_val_loss_fold[i]} - VAccuracy: {best_val_acc_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Training Accuracy: {np.mean(best_train_acc_fold)} (+- {np.std(best_train_acc_fold)})')\n",
    "print(f'> Training Loss: {np.mean(best_train_loss_fold)} (+- {np.std(best_train_loss_fold)})')\n",
    "print(f'> Validation Accuracy: {np.mean(best_val_acc_fold)} (+- {np.std(best_val_acc_fold)})')\n",
    "print(f'> Validation Loss: {np.mean(best_val_loss_fold)} (+- {np.std(best_val_loss_fold)})')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save training statistics"
   ],
   "metadata": {
    "id": "u3l95jSr5unH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "s = \"\"\" ------------------------------------------------------------------------\n",
    "Score per fold\n",
    "------------------------------------------------------------------------\n",
    "> Fold 1 - TLoss: 0.06644788384437561 - TAccuracy: 0.998160183429718%\n",
    "> Fold 1 - VLoss: 0.0760158970952034 - VAccuracy: 0.9977272748947144%\n",
    "------------------------------------------------------------------------\n",
    "> Fold 2 - TLoss: 0.04653624817728996 - TAccuracy: 0.9995659589767456%\n",
    "> Fold 2 - VLoss: 0.06799011677503586 - VAccuracy: 0.9987521767616272%\n",
    "------------------------------------------------------------------------\n",
    "> Fold 3 - TLoss: 0.05929543823003769 - TAccuracy: 0.9968156218528748%\n",
    "> Fold 3 - VLoss: 0.18274760246276855 - VAccuracy: 0.9939550757408142%\n",
    "------------------------------------------------------------------------\n",
    "> Fold 4 - TLoss: 0.05379674956202507 - TAccuracy: 0.9990285038948059%\n",
    "> Fold 4 - VLoss: 0.1970527321100235 - VAccuracy: 0.9954123497009277%\n",
    "------------------------------------------------------------------------\n",
    "> Fold 5 - TLoss: 0.046857770532369614 - TAccuracy: 0.9990285038948059%\n",
    "> Fold 5 - VLoss: 0.12490435689687729 - VAccuracy: 0.995088517665863%\n",
    "------------------------------------------------------------------------\n",
    "Average scores for all folds:\n",
    "> Training Accuracy: 0.99851975440979 (+- 0.000963904054787102)\n",
    "> Training Loss: 0.05458681806921959 (+- 0.007589862249069231)\n",
    "> Validation Accuracy: 0.9961870789527894 (+- 0.001774306115726234)\n",
    "> Validation Loss: 0.1297421410679817 (+- 0.053034932931961924)\n",
    "------------------------------------------------------------------------\"\"\""
   ],
   "metadata": {
    "id": "_ZsePnzX6Jg1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "training_data = {}\n",
    "\n",
    "training_data['summary'] = s\n",
    "training_data['best_training_losses'] = best_train_loss_fold\n",
    "training_data['best_validation_losses'] = best_val_loss_fold\n",
    "training_data['best_training_acc'] = best_train_acc_fold\n",
    "training_data['best_validation_acc'] = best_val_acc_fold\n",
    "\n",
    "\n",
    "with open('data_for_report/kfold_eff_svm_training_stats.json', 'w') as fp:\n",
    "    json.dump(training_data, fp)"
   ],
   "metadata": {
    "id": "BH5vDF8D5t_-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load trained models"
   ],
   "metadata": {
    "id": "3CSHfIAO4y8l"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNrb1QyZLnZK"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# load models \n",
    "total_model = 5\n",
    "model_directory = './'\n",
    "trained_models = list()\n",
    "for model_n in range(total_model):\n",
    "  filename = 'svm_kFold_' + str(model_n+1)\n",
    "  filename_chosen = os.path.join(model_directory, filename)\n",
    "  svm_model_loaded = load_model(filename_chosen + '.h5')\n",
    "  svm_model_loaded.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                    metrics=METRICS)\n",
    "  trained_models.append(svm_model_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate trained models (for storing statistics)"
   ],
   "metadata": {
    "id": "qSQrS3nR4387"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "validation_stats = {}\n",
    "plt.get_current_fig_manager().full_screen_toggle() # toggle fullscreen mode\n",
    "\n",
    "for idx, svm_model in enumerate(trained_models):  \n",
    "  predictions = svm_model.predict(test_gen)\n",
    "  y = np.argmax(tfk.utils.to_categorical(list(test.label))[:-2], axis = -1)\n",
    "  pred = np.argmax(predictions, axis=-1)\n",
    "  target_names = ['N', 'P', 'T']\n",
    "  cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "  # Compute the classification metrics\n",
    "  accuracy = accuracy_score(y, pred)\n",
    "  precision = precision_score(y, pred, average='macro')\n",
    "  recall = recall_score(y, pred, average='macro')\n",
    "  f1 = f1_score(y, pred, average='macro')\n",
    "\n",
    "\n",
    "  validation_stats['fold_' + str(idx+1) ] = {\n",
    "      'accuracy' : accuracy,\n",
    "      'precision' :precision,\n",
    "      'recall' :recall,\n",
    "      'f1' :f1,\n",
    "      'classification_report' :classification_report(y, pred, target_names=target_names, digits=4, output_dict = True)}\n",
    "\n",
    "\n",
    "  print('Accuracy:',accuracy.round(4))\n",
    "  print('Precision:',precision.round(4))\n",
    "  print('Recall:',recall.round(4))\n",
    "  print('F1:',f1.round(4))\n",
    "  print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "  # Plot the confusion matrix\n",
    "  plt.figure(figsize=(10,10))\n",
    "  hm = sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "  plt.xlabel('True labels')\n",
    "  plt.ylabel('Predicted labels')\n",
    "  plt.show()\n",
    "\n",
    "  hm.get_figure().savefig(\"data_for_report/heatmap_eff_svm_fold_\"+str(idx+1)+\".pdf\")\n",
    "\n",
    "with open('data_for_report/test_stats_eff_svm.json' , 'w') as fp:\n",
    "    json.dump(validation_stats, fp)\n",
    "\n"
   ],
   "metadata": {
    "id": "FAP0uKqZ42eZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8ZgDfFHL_Wf"
   },
   "source": [
    "## MAJORITY VOTING"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "preds = []\n",
    "\n",
    "for idx, svm_model in enumerate(trained_models):  \n",
    "  predictions = svm_model.predict(test_gen)\n",
    "  y = np.argmax(tfk.utils.to_categorical(list(test.label))[:-2], axis = -1)\n",
    "  pred = np.argmax(predictions, axis=-1)\n",
    "\n",
    "  preds.append(pred)"
   ],
   "metadata": {
    "id": "mPEulpTBCd13"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "preds_mv = scipy.stats.mode(preds, axis=0)[0][0]"
   ],
   "metadata": {
    "id": "0_IRea7dG6GN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(preds_mv)"
   ],
   "metadata": {
    "id": "JogT_sfuHOcZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y = np.argmax(tfk.utils.to_categorical(list(test.label))[:-2], axis = -1)\n",
    "pred = preds_mv\n",
    "target_names = ['N', 'P', 'T']\n",
    "cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(y, pred)\n",
    "precision = precision_score(y, pred, average='macro')\n",
    "recall = recall_score(y, pred, average='macro')\n",
    "f1 = f1_score(y, pred, average='macro')\n",
    "\n",
    "test_stats = {\n",
    "      'accuracy' : accuracy,\n",
    "      'precision' :precision,\n",
    "      'recall' :recall,\n",
    "      'f1' :f1,\n",
    "      'classification_report' :classification_report(y, pred, target_names=target_names, digits=4, output_dict = True)}\n",
    "\n",
    "\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "hm = sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()\n",
    "\n",
    "hm.get_figure().savefig(\"data_for_report/eff_svm_ensemble_heatmap.pdf\")\n",
    "\n",
    "with open('data_for_report/test_stats_ensemble_eff_svm.json' , 'w') as fp:\n",
    "    json.dump(test_stats, fp)"
   ],
   "metadata": {
    "id": "nPc00iF9HmUE"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xoHZ78wY7FC0",
    "UM-24HHW79aS",
    "NB-hx4_q-C1O",
    "7lrNseD9kmY_",
    "cSGtqSaGk8pN",
    "s0tcss01lmHT",
    "w8ZgDfFHL_Wf"
   ],
   "provenance": [],
   "gpuClass": "premium"
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
