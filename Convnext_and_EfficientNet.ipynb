{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0lxmAUxHKXK"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMsvvZPgLh73"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10.1  -q gwpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmE6ljAYwxZc"
   },
   "outputs": [],
   "source": [
    "!pip uninstall matplotlib\n",
    "!pip install matplotlib==3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXKKnFzU7hkY"
   },
   "outputs": [],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_S9EdNUBIps"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "from google.colab import drive\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array,array_to_img\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.callbacks import *\n",
    "import random\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(tf.__version__)\n",
    "import albumentations as A\n",
    "from itertools import combinations\n",
    "import json\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from typing import Tuple\n",
    "import scipy\n",
    "from skimage.measure import label as label_fn\n",
    "from skimage import filters\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7fORGxiP082"
   },
   "source": [
    "# Mount the My Drive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive', force_remount = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Jg65s02MorE"
   },
   "source": [
    "# Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-89ZrJjLgSH"
   },
   "outputs": [],
   "source": [
    "%cd /gdrive/MyDrive/tuberculosis-pneumonia-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JGdTB8uL8-t"
   },
   "outputs": [],
   "source": [
    "SEED = 4224\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "labels_path = 'data/labels_train_clean.csv'\n",
    "all_data_no_duplicates_path = 'data/train_all_no_duplicates'\n",
    "clean_data_path = 'data/train_clean/'\n",
    "noisy_data_path = 'data/train_noisy/'\n",
    "\n",
    "train_percentage = 0.8\n",
    "validation_percentage = 0.15\n",
    "test_percentage = 0.2\n",
    "img_size = (224,224)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91WMKvnXQqzz"
   },
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFZSc5-ilA2f"
   },
   "outputs": [],
   "source": [
    "class CustomGenerator(tf.keras.utils.Sequence):\n",
    "  \"\"\"\n",
    "    CustomGenerator inheriting from tf.keras.utils.Sequence.\n",
    "\n",
    "    We have to implement 3 main methods:\n",
    "      - __init__: save dataset params like directory, filenames, etc.\n",
    "      - __len__: return the total number of samples in the dataset (number of batches)\n",
    "      - __getitem__: return a single batch of paired images masks\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, \n",
    "               dataframe, # dataframe of the dataset  \n",
    "               base_path,\n",
    "               preprocessing_function=None, # Preprocessing function (e.g., the one used for transfer learning)\n",
    "               batch_size=16, # Batch size\n",
    "               out_shape = (100,100),\n",
    "               shuffle=False,\n",
    "               categorical = True,\n",
    "               augment = False,\n",
    "               seed = SEED,\n",
    "               flow_from_directory = True,\n",
    "               preprocess_input = False):\n",
    "    \n",
    "    # Get all filenames\n",
    "    if isinstance(base_path, Tuple):\n",
    "      self.filenames = []\n",
    "      for p in base_path:\n",
    "\n",
    "        paths = self.folderToPaths(p, full_path = False)\n",
    "\n",
    "        for pa in paths:\n",
    "          if pa in set(dataframe.file):\n",
    "            self.filenames.append(os.path.join(p, pa))\n",
    "\n",
    "\n",
    "    else:\n",
    "        self.filenames = [os.path.join(base_path, img_path) for img_path in list(dataframe.file)]\n",
    "\n",
    "    self.labels = tfk.utils.to_categorical(list(dataframe.label)) if categorical else list(dataframe.label)\n",
    "\n",
    "    # Set indices list in [0, len(subset_filenames)]\n",
    "    self.indices = np.arange(len(self.filenames))\n",
    "\n",
    "    # Save dataset parameters as class attributes\n",
    "    self.base_path = base_path\n",
    "    self.preprocessing_function = preprocessing_function\n",
    "    self.out_shape = out_shape\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = shuffle\n",
    "    self.augment = augment\n",
    "    self.seed = seed\n",
    "    self.flow_from_directory =flow_from_directory\n",
    "    self.data_augmentation = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit = 0.05, contrast_limit=0.05, p=0.5),\n",
    "    A.ShiftScaleRotate(p = 0.8, rotate_limit = 20, scale_limit = 0.3, border_mode =  cv2.BORDER_CONSTANT, value = 0),\n",
    "    A.CLAHE(p=0.2)\n",
    "    ])\n",
    "    self.preprocess_input = preprocess_input\n",
    "\n",
    "    if not self.flow_from_directory:\n",
    "      self.images = self.load_all_imgs()\n",
    "\n",
    "  def augmentation(self, images):\n",
    "    return self.data_augmentation(image = images)\n",
    "\n",
    "\n",
    "  def __filterNoisyOnClahe(self, image):\n",
    "    clahe = cv2.createCLAHE(clipLimit = 300, tileGridSize = (50, 50))\n",
    "    im1 = cv2.resize(image, (400, 400))\n",
    "    im1 = scipy.ndimage.gaussian_laplace(im1, sigma = 6)\n",
    "    im1 = clahe.apply(im1)\n",
    "    var1 = np.var(im1)\n",
    "    if var1 > 800:\n",
    "      image= cv2.medianBlur(image, ksize=5)\n",
    "      return scipy.ndimage.uniform_filter(image, size=3)\n",
    "    else:\n",
    "      return image\n",
    "\n",
    "\n",
    "  def __sharpenImage(self, image):\n",
    "    sharpen_kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "\n",
    "    sharpened = cv2.filter2D(image, -1, sharpen_kernel)\n",
    "\n",
    "    return sharpened\n",
    "\n",
    "  def __invert_image(self, img):\n",
    "\n",
    "    otsu_thresh = filters.threshold_otsu(img)\n",
    "    masked_image = (img > otsu_thresh) * 1.0\n",
    "    valsROI1, _ = np.histogram(masked_image[220:244, 220:244], bins=2, range=(0, 1))\n",
    "    valsROI2, _ = np.histogram(masked_image[0:25, 0:25], bins=2, range=(0, 1))\n",
    "    valsROI3, _ = np.histogram(masked_image[0:25, 220:244], bins=2, range=(0, 1))\n",
    "    valsROI4, _ = np.histogram(masked_image[220:244, 0:25], bins=2, range=(0, 1))\n",
    "    \n",
    "    valsTot = valsROI1 + valsROI2 + valsROI3 + valsROI4\n",
    "    \n",
    "    labels = label_fn(masked_image)\n",
    "\n",
    "    if len(np.unique(labels)) < 100:\n",
    "        if valsTot[0] > valsTot[1]:\n",
    "            return img\n",
    "        else:\n",
    "            return 255 - img\n",
    "    return img\n",
    "\n",
    "\n",
    "  def __filterBlurred(self, image):\n",
    "    clahe = cv2.createCLAHE(clipLimit = 1.8, tileGridSize = (4, 4))\n",
    "    minThresh = 2\n",
    "    im1 = cv2.resize(image, (400, 400))\n",
    "    im1 = scipy.ndimage.gaussian_laplace(im1, sigma = 2)\n",
    "    im1 = clahe.apply(im1)\n",
    "    _, im1 = cv2.threshold(im1, minThresh, 255, cv2.THRESH_BINARY)\n",
    "    var1 = np.var(im1)\n",
    "    if var1 < 5:\n",
    "        return self.__sharpenImage(image)\n",
    "    else:\n",
    "        return image \n",
    "\n",
    "  def __filterUnderexposed(self, image):\n",
    "    im1 = cv2.resize(image, (400, 400))\n",
    "    mean1 = np.mean(im1)\n",
    "    if mean1 < 71:\n",
    "      clahe = cv2.createCLAHE(clipLimit = 2, tileGridSize = (2, 2))\n",
    "      image = clahe.apply(image)\n",
    "      return image\n",
    "    else:\n",
    "      return image\n",
    "\n",
    "\n",
    "  def preprocess(self, image):\n",
    "\n",
    "    image = self.__invert_image(image)\n",
    "    image = self.__filterUnderexposed(image)\n",
    "    image = self.__filterNoisyOnClahe(image)\n",
    "    image = self.__filterBlurred(image)\n",
    "   \n",
    "\n",
    "    return image\n",
    "\n",
    "  def __len__(self):\n",
    "    # Return the length of the dataset (number of batches)\n",
    "    # that is given by #images // batch_size\n",
    "    return len(self.filenames) // self.batch_size\n",
    "\n",
    "  def on_epoch_start(self):\n",
    "    # Shuffle indices after each epoch\n",
    "    if self.shuffle == True:\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "  def load_all_imgs(self):\n",
    "      images = []\n",
    "      for f in self.filenames:\n",
    "        image = cv2.imread(f, 0)\n",
    "        image = cv2.resize(image, (self.out_shape))\n",
    "        if self.preprocess_input:\n",
    "          image = self.preprocess(image)\n",
    "        images.append(image)\n",
    "\n",
    "      return np.array(images)\n",
    "\n",
    "  def get_image_and_label(self, index):\n",
    "\n",
    "    if not self.flow_from_directory:\n",
    "      image = self.images[index]\n",
    "      if self.augment:\n",
    "        image = self.augmentation(image)\n",
    "      image = np.squeeze(image)\n",
    "      image = np.stack([image, image, image], axis = -1)\n",
    "      curr_label = self.labels[index]\n",
    "    else:\n",
    "      curr_filename = self.filenames[index] # Get filename at index\n",
    "      curr_label = self.labels[index]\n",
    "      image = cv2.imread(curr_filename, 0)\n",
    "      image = cv2.resize(image, (self.out_shape))\n",
    "      if self.preprocess_input:\n",
    "        image = self.preprocess(image)\n",
    "\n",
    "      if self.augment:\n",
    "        image = self.augmentation(image)['image']\n",
    "\n",
    "      image = np.stack([image, image, image], axis = -1)\n",
    "\n",
    "\n",
    "    return image, curr_label\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # In this function we generate a batch (of size self.batch_size) of images and corresponding masks\n",
    "    \n",
    "    # Get 'self.batch_size' indices\n",
    "    current_indices = self.indices[index*self.batch_size:(index*self.batch_size)+self.batch_size]\n",
    "\n",
    "    \"\"\"if len(current_indices) == 0:\n",
    "      current_indices = self.indices[len(self.indices)-self.batch_size:len(self.indices)]\"\"\"\n",
    "\n",
    "    # Init lists that will contain images and masks\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "\n",
    "    # Cycle over the indices\n",
    "    for idx in current_indices:\n",
    "      # Get single image/mask at index 'idx'\n",
    "      image, label = self.get_image_and_label(idx)\n",
    "\n",
    "      # Apply the preprocessing function\n",
    "      if self.preprocessing_function is not None:\n",
    "        image = self.preprocessing_function(image)\n",
    "\n",
    "      # Append both image and mask (with added batch dimension) to the corresponding batch lists\n",
    "      batch_images.append(np.expand_dims(image, 0))\n",
    "      batch_labels.append(label)\n",
    "     \n",
    "    # Finally, obtain a final batch by concatenating all the images over the batch dimension\n",
    "    batch_images = np.concatenate(batch_images, axis=0)\n",
    "    batch_labels = np.array(batch_labels)\n",
    "\n",
    "    return batch_images, batch_labels\n",
    "\n",
    "\n",
    "  def folderToPaths(\n",
    "        self,\n",
    "        full_img_dir,\n",
    "        full_path = True\n",
    "):\n",
    "\n",
    "    x_paths_list = []\n",
    "\n",
    "    full_img_dir = full_img_dir\n",
    "\n",
    "    for full in os.listdir(full_img_dir):\n",
    "         if full_path:\n",
    "            x_paths_list.append(os.path.join(full_img_dir, full))\n",
    "         else:\n",
    "          x_paths_list.append(full)\n",
    "    \n",
    "    x_paths_list.sort()\n",
    "    return x_paths_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRwAWbxkLrMl"
   },
   "source": [
    "# Cyclical LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1O985rULtF1"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLS05gdIbOPK"
   },
   "source": [
    "# ConvNext Experiment 1: train with all data (no duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7BzDdztYjjT"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhiNznfVPa_C"
   },
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "  if x == 'N':\n",
    "    return 0\n",
    "  elif x == 'P':\n",
    "    return 1\n",
    "  else:\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeKrCQFNV3vj"
   },
   "outputs": [],
   "source": [
    "def folderToPaths(\n",
    "        full_img_dir,\n",
    "        full_path = True\n",
    "):\n",
    "\n",
    "    x_paths_list = []\n",
    "\n",
    "    full_img_dir = full_img_dir\n",
    "\n",
    "    for full in os.listdir(full_img_dir):\n",
    "         if full_path:\n",
    "            x_paths_list.append(os.path.join(full_img_dir, full))\n",
    "         else:\n",
    "          x_paths_list.append(full)\n",
    "    \n",
    "    x_paths_list.sort()\n",
    "    return x_paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VkrXh1hbupM"
   },
   "outputs": [],
   "source": [
    "labelsDF = pd.read_csv(labels_path)\n",
    "display(labelsDF.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CG2JL2DvNNaN"
   },
   "outputs": [],
   "source": [
    "labelsDF.label = labelsDF.label.apply(lambda x: encode(x))\n",
    "display(labelsDF.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmbBIroFfOuJ"
   },
   "outputs": [],
   "source": [
    "len(set(labelsDF.file)) # 1 acquisition per patienty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yCyi2zTas8E"
   },
   "outputs": [],
   "source": [
    "all_data_no_duplicates_path_list = folderToPaths(full_img_dir = all_data_no_duplicates_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IiwDRRUBoTHo"
   },
   "outputs": [],
   "source": [
    "train_val, test = train_test_split(labelsDF, test_size = test_percentage, shuffle = True, stratify = labelsDF.label, random_state = SEED)\n",
    "train, val = train_test_split(train_val, test_size = validation_percentage, shuffle = True, stratify = train_val.label, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q54SWZ6JGjCU"
   },
   "outputs": [],
   "source": [
    "train_gen = CustomGenerator(dataframe = train, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True)\n",
    "valid_gen = CustomGenerator(dataframe = val, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True)\n",
    "test_gen = CustomGenerator(dataframe = test, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oGrLmoweZgF"
   },
   "outputs": [],
   "source": [
    "dataset_labels = np.array(list(set(labelsDF.label)), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SJuXbEQJToO"
   },
   "outputs": [],
   "source": [
    "iterator = iter(train_gen)\n",
    "images, labels = next(iterator)\n",
    "fig, axis = plt.subplots(4, 4, figsize = (20, 20))\n",
    "\n",
    "axis = axis.flatten()\n",
    "\n",
    "for i in range(images.shape[0]):\n",
    "  axis[i].imshow(images[i].squeeze(), cmap='gray')\n",
    "  axis[i].set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Wf2FyWTLkyJ"
   },
   "source": [
    "## Transfer learning model - no augmentation - no class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUW19jexKgkJ"
   },
   "outputs": [],
   "source": [
    "supernet1 = tf.keras.applications.convnext.ConvNeXtTiny(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "\n",
    "count = 1\n",
    "print(len(supernet1.layers))\n",
    "for layer in supernet1.layers:\n",
    "    if count < 80:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Wr0dEbULIh9"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D, MaxPooling2D, BatchNormalization,Concatenate, Resizing\n",
    "from keras import regularizers\n",
    "\n",
    "inputs = tfk.Input((224,224,3))\n",
    "\n",
    "y = supernet1(inputs)\n",
    "\n",
    "y = tf.keras.layers.GlobalAveragePooling2D()(y)\n",
    "\n",
    "y = tf.keras.layers.Dense(256, activation='relu', kernel_initializer = tfk.initializers.HeUniform(SEED))(y)\n",
    "\n",
    "outputs2 = tf.keras.layers.Dense(3, activation='softmax', kernel_initializer = tfk.initializers.GlorotUniform(SEED))(y)\n",
    "\n",
    "tl_model_exp1 = tfk.Model(inputs=inputs, outputs=outputs2, name='model')\n",
    "\n",
    "tl_model_exp1.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate = 1e-4), metrics='accuracy')\n",
    "tl_model_exp1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9m6n4YIELYoD"
   },
   "outputs": [],
   "source": [
    "training_samples = int(len(train_gen)*batch_size)\n",
    "step_size = 6*training_samples // batch_size\n",
    "\n",
    "clr = CyclicLR(\n",
    "    mode='triangular',\n",
    "    base_lr=1e-5, \n",
    "    max_lr=1e-4,\n",
    "    step_size= step_size)\n",
    "\n",
    "history = tl_model_exp1.fit(train_gen,\n",
    "    epochs = 40,\n",
    "    use_multiprocessing = True,\n",
    "    workers = 8,\n",
    "    validation_data = valid_gen,\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor= 'val_accuracy', mode='max', patience=10, restore_best_weights=True), clr],\n",
    ").history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5dRvlaTj67R"
   },
   "source": [
    "### Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvOmJiv2bMrR"
   },
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Category Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "with open('data_for_report/cnext_exp1_historyj.json' , 'w') as fp:\n",
    "    json.dump(history, fp)"
   ],
   "metadata": {
    "id": "At0G3uAYW7FD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDz2aYLDdLnZ"
   },
   "outputs": [],
   "source": [
    "tl_model_exp1.save(\"model_exp1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaYi4oHoa0bQ"
   },
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLtN3LPea57W"
   },
   "outputs": [],
   "source": [
    "# Predict the test set with the CNN\n",
    "predictions = tl_model_exp1.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTnHjugSbJ3f"
   },
   "outputs": [],
   "source": [
    "y = np.argmax(tfk.utils.to_categorical(list(test.label))[:-2], axis = -1)\n",
    "pred = np.argmax(predictions, axis=-1)\n",
    "target_names = ['N', 'P', 'T']\n",
    "cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(y, pred)\n",
    "precision = precision_score(y, pred, average='macro')\n",
    "recall = recall_score(y, pred, average='macro')\n",
    "f1 = f1_score(y, pred, average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnHazBUfRRBG"
   },
   "source": [
    "## Transfer learning model - no augmentation - class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--xFUMT7gLpj"
   },
   "outputs": [],
   "source": [
    "train_gen = CustomGenerator(dataframe = train, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, categorical = False, flow_from_directory=False)\n",
    "valid_gen = CustomGenerator(dataframe = val, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, categorical = False, flow_from_directory=False)\n",
    "test_gen = CustomGenerator(dataframe = test, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, categorical = False, flow_from_directory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5OZXEyUeFn4"
   },
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 classes = dataset_labels,\n",
    "                                                 y = train.label)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7gNQZ82RDXu"
   },
   "outputs": [],
   "source": [
    "supernet1 = tf.keras.applications.convnext.ConvNeXtTiny(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "\n",
    "count = 1\n",
    "print(len(supernet1.layers))\n",
    "for layer in supernet1.layers:\n",
    "    if count < 80:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install focal-loss"
   ],
   "metadata": {
    "id": "CZtdAAhTXQ4n"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from focal_loss import SparseCategoricalFocalLoss"
   ],
   "metadata": {
    "id": "ye1nZ0tUXS3Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJXoyEEWVt2h"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D, MaxPooling2D, BatchNormalization,Concatenate, Resizing\n",
    "from keras import regularizers\n",
    "\n",
    "inputs = tfk.Input((224,224,3))\n",
    "\n",
    "\n",
    "y = supernet1(inputs)\n",
    "\n",
    "y = tf.keras.layers.GlobalAveragePooling2D()(y)\n",
    "\n",
    "y = tf.keras.layers.Dense(256, activation='relu', kernel_initializer = tfk.initializers.HeUniform(SEED))(y)\n",
    "\n",
    "outputs2 = tf.keras.layers.Dense(3, activation='softmax', kernel_initializer = tfk.initializers.GlorotUniform(SEED))(y)\n",
    "\n",
    "tl_model_exp2 = tfk.Model(inputs=inputs, outputs=outputs2, name='model')\n",
    "\n",
    "tl_model_exp2.compile(loss=SparseCategoricalFocalLoss(class_weight = class_weights, gamma=2), optimizer=tfk.optimizers.Adam(learning_rate = 1e-4), metrics='accuracy')\n",
    "\n",
    "tl_model_exp2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6T6iQWyQgeT6"
   },
   "outputs": [],
   "source": [
    "training_samples = int(len(train_gen)*batch_size)\n",
    "step_size = 6*training_samples // batch_size\n",
    "\n",
    "clr = CyclicLR(\n",
    "    mode='triangular',\n",
    "    base_lr=1e-5, \n",
    "    max_lr=1e-4,\n",
    "    step_size= step_size)\n",
    "\n",
    "history = tl_model_exp2.fit(train_gen,\n",
    "    epochs = 50,\n",
    "    use_multiprocessing = True,\n",
    "    workers = 8,\n",
    "    validation_data = valid_gen,\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor= 'val_accuracy', mode='max', patience=15, restore_best_weights=True), clr],\n",
    ").history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGaQeU3hosst"
   },
   "source": [
    "### Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjoR8wXKosst"
   },
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Category Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "with open('data_for_report/cnext_exp2_historyj.json' , 'w') as fp:\n",
    "    json.dump(history, fp)"
   ],
   "metadata": {
    "id": "cZ5O-ym0Xsg4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1zf6FHWjtdb"
   },
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBvR2FV9kG8a"
   },
   "outputs": [],
   "source": [
    "# Predict the test set with the CNN\n",
    "predictions = tl_model_exp2.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qd10ePJNjrm2"
   },
   "outputs": [],
   "source": [
    "y = np.argmax(tfk.utils.to_categorical(list(test.label))[:-2], axis = -1)\n",
    "pred = np.argmax(predictions, axis=-1)\n",
    "target_names = ['N', 'P', 'T']\n",
    "cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(y, pred)\n",
    "precision = precision_score(y, pred, average='macro')\n",
    "recall = recall_score(y, pred, average='macro')\n",
    "f1 = f1_score(y, pred, average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhRb4aO-haM6"
   },
   "source": [
    "## Transfer learning model - augmentation - no class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfDpWdrbqiHk"
   },
   "outputs": [],
   "source": [
    "train_gen = CustomGenerator(dataframe = train, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, categorical = True, augment = True)\n",
    "valid_gen = CustomGenerator(dataframe = val, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, categorical = True, augment = True)\n",
    "test_gen = CustomGenerator(dataframe = test, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, categorical = True, augment = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bQmgg8dq9vB"
   },
   "outputs": [],
   "source": [
    "supernet1 = tf.keras.applications.convnext.ConvNeXtTiny(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "\n",
    "count = 1\n",
    "print(len(supernet1.layers))\n",
    "for layer in supernet1.layers:\n",
    "    if count < 80:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZYqal7NrEqK"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D, MaxPooling2D, BatchNormalization,Concatenate, Resizing\n",
    "from keras import regularizers\n",
    "\n",
    "inputs = tfk.Input(shape = (224,224,3))\n",
    "\n",
    "y = supernet1(inputs)\n",
    "\n",
    "\n",
    "\n",
    "y = tf.keras.layers.GlobalAveragePooling2D()(y)\n",
    "\n",
    "y = tf.keras.layers.Dense(256, activation='relu', kernel_initializer = tfk.initializers.HeUniform(SEED))(y)\n",
    "\n",
    "outputs2 = tf.keras.layers.Dense(3, activation='softmax', kernel_initializer = tfk.initializers.GlorotUniform(SEED))(y)\n",
    "\n",
    "tl_model = tfk.Model(inputs=inputs, outputs=outputs2, name='model')\n",
    "\n",
    "tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate = 1e-4), metrics='accuracy')\n",
    "\n",
    "tl_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tc_kdDJsIlM"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvDIHgCwrKkY"
   },
   "outputs": [],
   "source": [
    "training_samples = int(len(train_gen)*batch_size)\n",
    "step_size = 6*training_samples // batch_size\n",
    "\n",
    "clr = CyclicLR(\n",
    "    mode='triangular',\n",
    "    base_lr=1e-5, \n",
    "    max_lr=1e-4,\n",
    "    step_size= step_size)\n",
    "\n",
    "history = tl_model.fit(train_gen,\n",
    "    epochs = 55,\n",
    "    workers = 8,\n",
    "    use_multiprocessing = True,\n",
    "    validation_data = valid_gen,\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor= 'val_accuracy', mode='max', patience=20, restore_best_weights=True), clr],\n",
    ").history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv32XdvR7znb"
   },
   "source": [
    "### Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uj6ZSJHX7zSh"
   },
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Category Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "with open('data_for_report/cnext_exp3_historyj.json' , 'w') as fp:\n",
    "    json.dump(history, fp)"
   ],
   "metadata": {
    "id": "dKelvEloYPwf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNASXl0G743g"
   },
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-nHXU5r743g"
   },
   "outputs": [],
   "source": [
    "# Predict the test set with the CNN\n",
    "predictions = tl_model.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zirsO9vA743h"
   },
   "outputs": [],
   "source": [
    "y = np.argmax(tfk.utils.to_categorical(list(test.label))[:-2], axis = -1)\n",
    "pred = np.argmax(predictions, axis=-1)\n",
    "target_names = ['N', 'P', 'T']\n",
    "cm = confusion_matrix(y, pred)\n",
    "\n",
    "\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(y, pred)\n",
    "precision = precision_score(y, pred, average='macro')\n",
    "recall = recall_score(y, pred, average='macro')\n",
    "f1 = f1_score(y, pred, average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOud8zgm9HUH"
   },
   "source": [
    "# EffNet Experiment: train with data preprocessing + 1vsAll\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsZlfAmy5RKi"
   },
   "source": [
    "## Common steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tFNiy3P9HUS"
   },
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "  if x == 'T':\n",
    "    return 0\n",
    "  elif x == 'P':\n",
    "    return 1\n",
    "  else:\n",
    "    return 2\n",
    "\n",
    "def encode2(x, target_label):\n",
    "  if x == target_label:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07UxHCl99HUT"
   },
   "outputs": [],
   "source": [
    "def folderToPaths(\n",
    "        full_img_dir,\n",
    "        full_path = True\n",
    "):\n",
    "\n",
    "    x_paths_list = []\n",
    "\n",
    "    full_img_dir = full_img_dir\n",
    "\n",
    "    for full in os.listdir(full_img_dir):\n",
    "         if full_path:\n",
    "            x_paths_list.append(os.path.join(full_img_dir, full))\n",
    "         else:\n",
    "          x_paths_list.append(full)\n",
    "    \n",
    "    x_paths_list.sort()\n",
    "    return x_paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-Xn3bth9HUT"
   },
   "outputs": [],
   "source": [
    "labelsDF = pd.read_csv(labels_path)\n",
    "display(labelsDF.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIwzyl2H9HUZ"
   },
   "outputs": [],
   "source": [
    "len(set(labelsDF.file)) # 1 acquisition per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCeeq9bC9HUa"
   },
   "outputs": [],
   "source": [
    "train_val, test = train_test_split(labelsDF, test_size = test_percentage, shuffle = True, stratify = labelsDF.label, random_state = SEED)\n",
    "train, val = train_test_split(train_val, test_size = validation_percentage, shuffle = True, stratify = train_val.label, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cJuD6YXGFg_"
   },
   "outputs": [],
   "source": [
    "train_ovaN = train.copy()\n",
    "train_ovaN.label =  train.label.apply(lambda x: encode2(x, 'N'))\n",
    "\n",
    "val_ovaN= val.copy()\n",
    "val_ovaN.label =  val.label.apply(lambda x: encode2(x, 'N'))\n",
    "\n",
    "test_ovaN = test.copy()\n",
    "test_ovaN.label =  test.label.apply(lambda x: encode2(x, 'N'))\n",
    "\n",
    "\n",
    "train_ovaT = train.copy()\n",
    "train_ovaT.label =  train.label.apply(lambda x: encode2(x, 'T'))\n",
    "\n",
    "val_ovaT= val.copy()\n",
    "val_ovaT.label =  val.label.apply(lambda x: encode2(x, 'T'))\n",
    "\n",
    "test_ovaT = test.copy()\n",
    "test_ovaT.label =  test.label.apply(lambda x: encode2(x, 'T'))\n",
    "\n",
    "\n",
    "train_ovaP = train.copy()\n",
    "train_ovaP.label =  train.label.apply(lambda x: encode2(x, 'P'))\n",
    "\n",
    "val_ovaP= val.copy()\n",
    "val_ovaP.label =  val.label.apply(lambda x: encode2(x, 'P'))\n",
    "\n",
    "test_ovaP = test.copy()\n",
    "test_ovaP.label =  test.label.apply(lambda x: encode2(x, 'P'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9al2BxmBJKr"
   },
   "outputs": [],
   "source": [
    "display(test_ovaT.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70H2APYm9HUa"
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(train.label, return_counts=True)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLLif51c9HUc"
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2KjKBktFqMV"
   },
   "outputs": [],
   "source": [
    "train_gen_ovaT = CustomGenerator(dataframe = train_ovaT, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False, augment = False)\n",
    "valid_gen_ovaT = CustomGenerator(dataframe = val_ovaT, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False,  augment = False)\n",
    "test_gen_ovaT = CustomGenerator(dataframe = test_ovaT, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = False, flow_from_directory=True, preprocess_input = True, categorical = False,  augment = False)\n",
    "\n",
    "train_gen_ovaN = CustomGenerator(dataframe = train_ovaN, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False)\n",
    "valid_gen_ovaN = CustomGenerator(dataframe = val_ovaN, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False)\n",
    "test_gen_ovaN = CustomGenerator(dataframe = test_ovaN, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = False, flow_from_directory=True, preprocess_input = True, categorical = False)\n",
    "\n",
    "train_gen_ovaP = CustomGenerator(dataframe = train_ovaP, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False)\n",
    "valid_gen_ovaP = CustomGenerator(dataframe = val_ovaP, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False)\n",
    "test_gen_ovaP =  CustomGenerator(dataframe = test_ovaP, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = False, flow_from_directory=True, preprocess_input = True, categorical = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoQzn6Df9HUd"
   },
   "outputs": [],
   "source": [
    "iterator = iter(train_gen_ovaT)\n",
    "images, labels = next(iterator)\n",
    "fig, axis = plt.subplots(4, 4, figsize = (20, 20))\n",
    "\n",
    "axis = axis.flatten()\n",
    "\n",
    "for i in range(16):\n",
    "  axis[i].imshow(images[i].squeeze(), cmap='gray')\n",
    "  axis[i].set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYL6mR0R9HUe"
   },
   "outputs": [],
   "source": [
    "iterator = iter(test_gen_ovaT)\n",
    "images, labels = next(iterator)\n",
    "fig, axis = plt.subplots(4, 4, figsize = (20, 20))\n",
    "\n",
    "axis = axis.flatten()\n",
    "\n",
    "for i in range(16):\n",
    "  axis[i].imshow(images[i].squeeze(), cmap='gray')\n",
    "  axis[i].set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcpuwKq13WM-"
   },
   "source": [
    "## First binary classifier: N and focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPXuOdQI-wWY"
   },
   "outputs": [],
   "source": [
    "supernet2 = tf.keras.applications.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "supernet2._name = \"effnet2\"\n",
    "\n",
    "\n",
    "\n",
    "count = 1\n",
    "print(len(supernet2.layers))\n",
    "for layer in supernet2.layers:\n",
    "    if count < 80:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzINBfG29HUf"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D, MaxPooling2D, BatchNormalization,Concatenate, Resizing\n",
    "from keras import regularizers\n",
    "\n",
    "inputs = tfk.Input((224,224,3))\n",
    "\n",
    "y2 = supernet2(inputs)\n",
    "\n",
    "y2 = tf.keras.layers.GlobalAveragePooling2D()(y2)\n",
    "y2 = tf.keras.layers.Dense(256, activation='relu', kernel_initializer = tfk.initializers.HeUniform(SEED))(y2)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer = tfk.initializers.GlorotUniform(SEED))(y2)\n",
    "\n",
    "# Connect input and output through the Model class\n",
    "tl_model_exp3 = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model\n",
    "tl_model_exp3.compile(loss=tfk.losses.BinaryFocalCrossentropy(True), optimizer=tfk.optimizers.Adam(learning_rate = 1e-4), metrics='accuracy')\n",
    "\n",
    "tl_model_exp3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXjrUSz-9HUf"
   },
   "outputs": [],
   "source": [
    "training_samples = int(len(train_gen_ovaN)*batch_size)\n",
    "step_size = 6*training_samples // batch_size\n",
    "\n",
    "clr = CyclicLR(\n",
    "    mode='triangular',\n",
    "    base_lr=1e-5, \n",
    "    max_lr=1e-4,             \n",
    "    step_size= step_size\n",
    "    )\n",
    "\n",
    "history = tl_model_exp3.fit(\n",
    "    train_gen_ovaN,\n",
    "    workers=8,\n",
    "    use_multiprocessing=True,\n",
    "    epochs = 50,\n",
    "    validation_data = valid_gen_ovaN,\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor= 'val_accuracy', mode='max', patience=10, restore_best_weights=True), clr],\n",
    ").history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XrM_0I39HUg"
   },
   "source": [
    "### Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCx4cj379HUg"
   },
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Category Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vr1yxnXO9HUg"
   },
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwkRT4639HUh"
   },
   "outputs": [],
   "source": [
    "# Predict the test set with the CNN\n",
    "predictions = tl_model_exp3.predict(test_gen_ovaN, use_multiprocessing = True, workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5V_k9uDdbztM"
   },
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJPuHscP9HUh"
   },
   "outputs": [],
   "source": [
    "y = list(test_ovaN.label)[:2368]\n",
    "pred = (predictions > 0.5).astype(int)\n",
    "target_names = ['O', 'N']\n",
    "cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(y, pred)\n",
    "precision = precision_score(y, pred, average='macro')\n",
    "recall = recall_score(y, pred, average='macro')\n",
    "f1 = f1_score(y, pred, average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "250abO719HUi"
   },
   "outputs": [],
   "source": [
    "tl_model_exp3.save(\"model_exp3_N2.h5\") # N2 is with focal loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9avvKyscpC0"
   },
   "source": [
    "## Second binary classifier: T and focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5UBbYZqcpC1"
   },
   "outputs": [],
   "source": [
    "supernet2 = tf.keras.applications.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "\n",
    "supernet2._name = \"effnet2\"\n",
    "\n",
    "\n",
    "\n",
    "count = 1\n",
    "print(len(supernet2.layers))\n",
    "for layer in supernet2.layers:\n",
    "    if count < 80:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BIdIuVvcpC1"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D, MaxPooling2D, BatchNormalization,Concatenate, Resizing\n",
    "from keras import regularizers\n",
    "\n",
    "inputs = tfk.Input((224,224,3))\n",
    "\n",
    "y2 = supernet2(inputs)\n",
    "\n",
    "y2 = tf.keras.layers.GlobalAveragePooling2D()(y2)\n",
    "y2 = tf.keras.layers.Dense(256, activation='relu', kernel_initializer = tfk.initializers.HeUniform(SEED))(y2)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer = tfk.initializers.GlorotUniform(SEED))(y2)\n",
    "\n",
    "# Connect input and output through the Model class\n",
    "tl_model_exp3 = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model\n",
    "tl_model_exp3.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(True), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "tl_model_exp3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKYdm9bZ1ZbJ"
   },
   "outputs": [],
   "source": [
    "tl_model_exp3.load_weights(\"/gdrive/MyDrive/tuberculosis-pneumonia-classification/model_exp3_N2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTSFAmWPcpC1"
   },
   "outputs": [],
   "source": [
    "training_samples = int(len(train_gen_ovaT)*batch_size)\n",
    "step_size = 6*training_samples // batch_size\n",
    "\n",
    "clr = CyclicLR(\n",
    "    mode='triangular',\n",
    "    base_lr=1e-5, \n",
    "    max_lr=1e-4,             \n",
    "    step_size= step_size\n",
    "    )\n",
    "\n",
    "history = tl_model_exp3.fit(\n",
    "    train_gen_ovaT,\n",
    "    workers=8,\n",
    "    use_multiprocessing=True,\n",
    "    epochs = 50,\n",
    "    validation_data = valid_gen_ovaT,\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor= 'val_accuracy', mode='max', patience=10, restore_best_weights=True), clr],\n",
    ").history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-MdwFGXcpC2"
   },
   "source": [
    "### Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cOIHnFucpC2"
   },
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Category Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ufec4XXQcpC2"
   },
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzF4D5vycpC2"
   },
   "outputs": [],
   "source": [
    "# Predict the test set with the CNN\n",
    "predictions = tl_model_exp3.predict(test_gen_ovaT,\n",
    "                                    workers=8,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhCm0rnicpC3"
   },
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yN2C4lXdcpC3"
   },
   "outputs": [],
   "source": [
    "y = list(test_ovaT.label)[:2368]\n",
    "pred = (predictions > 0.5).astype(int)\n",
    "target_names = ['O', 'T']\n",
    "cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(y, pred)\n",
    "precision = precision_score(y, pred, average='macro')\n",
    "recall = recall_score(y, pred, average='macro')\n",
    "f1 = f1_score(y, pred, average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDAKKPTdcpC3"
   },
   "outputs": [],
   "source": [
    "tl_model_exp3.save(\"model_exp3_T2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpRdz4nyloZT"
   },
   "source": [
    "## Third binary classifier: P and BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83cLLBQRloZT"
   },
   "outputs": [],
   "source": [
    "supernet2 = tf.keras.applications.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "supernet2._name = \"effnet2\"\n",
    "\n",
    "\n",
    "\n",
    "count = 1\n",
    "print(len(supernet2.layers))\n",
    "for layer in supernet2.layers:\n",
    "    if count < 80:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUPLDnyHloZU"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D, MaxPooling2D, BatchNormalization,Concatenate, Resizing\n",
    "from keras import regularizers\n",
    "\n",
    "inputs = tfk.Input((224,224,3))\n",
    "\n",
    "y2 = supernet2(inputs)\n",
    "\n",
    "y2 = tf.keras.layers.GlobalAveragePooling2D()(y2)\n",
    "y2 = tf.keras.layers.Dense(256, activation='relu', kernel_initializer = tfk.initializers.HeUniform(SEED))(y2)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer = tfk.initializers.GlorotUniform(SEED))(y2)\n",
    "\n",
    "# Connect input and output through the Model class\n",
    "tl_model_exp3 = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model\n",
    "tl_model_exp3.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(True), optimizer=tfk.optimizers.Adam(learning_rate = 1e-4), metrics='accuracy')\n",
    "\n",
    "tl_model_exp3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NP1-PDsPloZU"
   },
   "outputs": [],
   "source": [
    "tl_model_exp3.load_weights(\"/gdrive/MyDrive/tuberculosis-pneumonia-classification/model_exp3_N2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEnuflKUloZU"
   },
   "outputs": [],
   "source": [
    "training_samples = int(len(train_gen_ovaN)*batch_size)\n",
    "step_size = 6*training_samples // batch_size\n",
    "\n",
    "clr = CyclicLR(\n",
    "    mode='triangular',\n",
    "    base_lr=1e-5, \n",
    "    max_lr=1e-4,             \n",
    "    step_size= step_size\n",
    "    )\n",
    "\n",
    "history = tl_model_exp3.fit(\n",
    "    train_gen_ovaP,\n",
    "    workers=8,\n",
    "    use_multiprocessing=True,\n",
    "    epochs = 50,\n",
    "    validation_data = valid_gen_ovaP,\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor= 'val_accuracy', mode='max', patience=10, restore_best_weights=True), clr],\n",
    ").history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JPdeiV4loZU"
   },
   "source": [
    "### Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_G0rEBTloZV"
   },
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Category Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pk2ALIeeloZV"
   },
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpwOo9wYloZV"
   },
   "outputs": [],
   "source": [
    "# Predict the test set with the CNN\n",
    "predictions = tl_model_exp3.predict(test_gen_ovaP,workers=8,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjetD1B1loZV"
   },
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fTUejJHloZV"
   },
   "outputs": [],
   "source": [
    "y = list(test_ovaP.label)[:2368]\n",
    "pred = (predictions > 0.5).astype(int)\n",
    "target_names = ['O', '  P']\n",
    "cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(y, pred)\n",
    "precision = precision_score(y, pred, average='macro')\n",
    "recall = recall_score(y, pred, average='macro')\n",
    "f1 = f1_score(y, pred, average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2EljH16loZW"
   },
   "outputs": [],
   "source": [
    "tl_model_exp3.save(\"model_exp3_P2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycI0CjKh-J-c"
   },
   "source": [
    "# EffNext OvA Ensemble inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TH3Q-H3i-Siv"
   },
   "outputs": [],
   "source": [
    "supernet = tf.keras.applications.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "count = 1\n",
    "print(len(supernet.layers))\n",
    "for layer in supernet.layers:\n",
    "    if count < 80:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otIIChs--blz"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D, MaxPooling2D, BatchNormalization,Concatenate, Resizing\n",
    "from keras import regularizers\n",
    "\n",
    "inputs = tfk.Input((224,224,3))\n",
    "\n",
    "y2 = supernet(inputs)\n",
    "\n",
    "y2 = tf.keras.layers.GlobalAveragePooling2D()(y2)\n",
    "y2 = tf.keras.layers.Dense(256, activation='relu', kernel_initializer = tfk.initializers.HeUniform(SEED))(y2)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer = tfk.initializers.GlorotUniform(SEED))(y2)\n",
    "\n",
    "# Connect input and output through the Model class\n",
    "tl_model_exp = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model\n",
    "tl_model_exp.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate = 1e-4), metrics='accuracy')\n",
    "\n",
    "tl_model_exp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drOtw1At_KEk"
   },
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "  if x == 'N':\n",
    "    return 0\n",
    "  elif x == 'P':\n",
    "    return 1\n",
    "  else:\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAVpOfxM_KEl"
   },
   "outputs": [],
   "source": [
    "def folderToPaths(\n",
    "        full_img_dir,\n",
    "        full_path = True\n",
    "):\n",
    "\n",
    "    x_paths_list = []\n",
    "\n",
    "    full_img_dir = full_img_dir\n",
    "\n",
    "    for full in os.listdir(full_img_dir):\n",
    "         if full_path:\n",
    "            x_paths_list.append(os.path.join(full_img_dir, full))\n",
    "         else:\n",
    "          x_paths_list.append(full)\n",
    "    \n",
    "    x_paths_list.sort()\n",
    "    return x_paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCKPiC0Q_KEl"
   },
   "outputs": [],
   "source": [
    "labelsDF = pd.read_csv(labels_path)\n",
    "display(labelsDF.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "av8JkSPm_KEl"
   },
   "outputs": [],
   "source": [
    "labelsDF.label = labelsDF.label.apply(lambda x: encode(x))\n",
    "display(labelsDF.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ai1w-Xqw_KEm"
   },
   "outputs": [],
   "source": [
    "len(set(labelsDF.file)) # 1 acquisition per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ni0xaL6_KEm"
   },
   "outputs": [],
   "source": [
    "train_val, test = train_test_split(labelsDF, test_size = test_percentage, shuffle = True, stratify = labelsDF.label, random_state = SEED)\n",
    "train, val = train_test_split(train_val, test_size = validation_percentage, shuffle = True, stratify = train_val.label, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "at0E_ywS_KEm"
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReWxH672_KEm"
   },
   "outputs": [],
   "source": [
    "train_gen = CustomGenerator(dataframe = train, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False)\n",
    "valid_gen = CustomGenerator(dataframe = val, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False)\n",
    "test_gen = CustomGenerator(dataframe = test, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdfMaVww_YaB"
   },
   "outputs": [],
   "source": [
    "tl_model_exp.load_weights(\"/gdrive/MyDrive/tuberculosis-pneumonia-classification/model_exp3_P2.h5\")\n",
    "# Predict the test set with the CNN\n",
    "predictionsP = tl_model_exp.predict(test_gen,\n",
    "                                    workers=8,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIDZ9Iya_Qhj"
   },
   "outputs": [],
   "source": [
    "tl_model_exp.load_weights(\"/gdrive/MyDrive/tuberculosis-pneumonia-classification/model_exp3_N2.h5\")\n",
    "# Predict the test set with the CNN\n",
    "predictionsN = tl_model_exp.predict(test_gen,\n",
    "                                    workers=8,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUTY6FtkAHdw"
   },
   "outputs": [],
   "source": [
    "tl_model_exp.load_weights(\"/gdrive/MyDrive/tuberculosis-pneumonia-classification/model_exp3_T2.h5\")\n",
    "# Predict the test set with the CNN\n",
    "predictionsT = tl_model_exp.predict(test_gen,\n",
    "                                    workers=8,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfSWhCmZGNFb"
   },
   "outputs": [],
   "source": [
    "pred = np.squeeze(np.array([predictionsN, predictionsP, predictionsT]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCUwPJ6mGVsm"
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(pred, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "y = list(test.label)[:2368]\n",
    "target_names = ['N', 'P', 'T']\n",
    "cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(y, pred)\n",
    "precision = precision_score(y, pred, average='macro')\n",
    "recall = recall_score(y, pred, average='macro')\n",
    "f1 = f1_score(y, pred, average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "ZULugun1ZyZB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv_wmqcqKJor"
   },
   "source": [
    "# EffNet Experiment: train with data preprocessing + 1vsAll (KFOLD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nRAOXueKJos"
   },
   "source": [
    "## Common steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nv2p3DOvKJos"
   },
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "  if x == 'T':\n",
    "    return 0\n",
    "  elif x == 'P':\n",
    "    return 1\n",
    "  else:\n",
    "    return 2\n",
    "\n",
    "def encode2(x, target_label):\n",
    "  if x == target_label:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ny-KMSN7KJos"
   },
   "outputs": [],
   "source": [
    "def folderToPaths(\n",
    "        full_img_dir,\n",
    "        full_path = True\n",
    "):\n",
    "\n",
    "    x_paths_list = []\n",
    "\n",
    "    full_img_dir = full_img_dir\n",
    "\n",
    "    for full in os.listdir(full_img_dir):\n",
    "         if full_path:\n",
    "            x_paths_list.append(os.path.join(full_img_dir, full))\n",
    "         else:\n",
    "          x_paths_list.append(full)\n",
    "    \n",
    "    x_paths_list.sort()\n",
    "    return x_paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0pkqXJPKJos"
   },
   "outputs": [],
   "source": [
    "labelsDF = pd.read_csv(labels_path)\n",
    "display(labelsDF.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQeGKCRyKJot"
   },
   "outputs": [],
   "source": [
    "len(set(labelsDF.file)) # 1 acquisition per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Md09hhscKJot"
   },
   "outputs": [],
   "source": [
    "train_val, test = train_test_split(labelsDF, test_size = test_percentage, shuffle = True, stratify = labelsDF.label, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsvkS3_CKJou"
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6_SJYgoKJou"
   },
   "outputs": [],
   "source": [
    "train_gen_ovaT = CustomGenerator(dataframe = train_ovaT, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False, augment = False)\n",
    "valid_gen_ovaT = CustomGenerator(dataframe = val_ovaT, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False,  augment = False)\n",
    "test_gen_ovaT = CustomGenerator(dataframe = test_ovaT, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = False, flow_from_directory=True, preprocess_input = True, categorical = False,  augment = False)\n",
    "\n",
    "train_gen_ovaN = CustomGenerator(dataframe = train_ovaN, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False)\n",
    "valid_gen_ovaN = CustomGenerator(dataframe = val_ovaN, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False)\n",
    "test_gen_ovaN = CustomGenerator(dataframe = test_ovaN, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = False, flow_from_directory=True, preprocess_input = True, categorical = False)\n",
    "\n",
    "train_gen_ovaP = CustomGenerator(dataframe = train_ovaP, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False)\n",
    "valid_gen_ovaP = CustomGenerator(dataframe = val_ovaP, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False)\n",
    "test_gen_ovaP =  CustomGenerator(dataframe = test_ovaP, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = False, flow_from_directory=True, preprocess_input = True, categorical = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCXnrYoVKJou"
   },
   "source": [
    "## First binary classifier: N and focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vjpfkH6KJov"
   },
   "outputs": [],
   "source": [
    "def get_supernet():\n",
    "  supernet1 = tf.keras.applications.efficientnet_v2.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "\n",
    "  count = 1\n",
    "  for layer in supernet1.layers:\n",
    "      if count < 80:\n",
    "          layer.trainable = False\n",
    "      else:\n",
    "          layer.trainable = True\n",
    "      count = count + 1\n",
    "  \n",
    "  return supernet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK-zNt0vKJov"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D, MaxPooling2D, BatchNormalization,Concatenate, Resizing\n",
    "from keras import regularizers\n",
    "\n",
    "def build_model():\n",
    "\n",
    "  inputs = tfk.Input((224,224,3))\n",
    "\n",
    "  y2 = get_supernet()(inputs)\n",
    "\n",
    "  y2 = tf.keras.layers.GlobalAveragePooling2D()(y2)\n",
    "  y2 = tf.keras.layers.Dense(256, activation='relu', kernel_initializer = tfk.initializers.HeUniform(SEED))(y2)\n",
    "  outputs = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer = tfk.initializers.GlorotUniform(SEED))(y2)\n",
    "\n",
    "  # Connect input and output through the Model class\n",
    "  tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "  # Compile the model\n",
    "  tl_model.compile(loss=tfk.losses.BinaryFocalCrossentropy(True), optimizer=tfk.optimizers.Adam(learning_rate = 1e-4), metrics='accuracy')\n",
    "\n",
    "  return tl_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8iMo6UhLD0Q"
   },
   "outputs": [],
   "source": [
    "train_val_ovaN = train_val.copy()\n",
    "train_val_ovaN.label =  train_val.label.apply(lambda x: encode2(x, 'N'))\n",
    "test_ovaN = test.copy()\n",
    "test_ovaN.label =  test.label.apply(lambda x: encode2(x, 'N'))\n",
    "test_gen_ovaN = CustomGenerator(dataframe = test_ovaN, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = False, flow_from_directory=True, preprocess_input = True, categorical = False,  augment = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hGGotF3K80W"
   },
   "outputs": [],
   "source": [
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "val_acc_per_fold = []\n",
    "val_loss_per_fold = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = SEED)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(skf.split(train_val_ovaN.file.to_numpy(), train_val_ovaN.label.to_numpy())): \n",
    "  if i < 3:\n",
    "    continue\n",
    "  print(i)\n",
    "  trainDF = train_val_ovaN.filter(items= train_index, axis=0)\n",
    "  validDF = train_val_ovaN.filter(items= valid_index, axis=0)\n",
    "\n",
    "  train_gen = CustomGenerator(dataframe = trainDF, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False, augment = False)\n",
    "  valid_gen = CustomGenerator(dataframe = validDF, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = False, augment = False)\n",
    "\n",
    "  training_samples = int(len(train_gen)*batch_size)\n",
    "  step_size = 6*training_samples // batch_size\n",
    "\n",
    "  clr = CyclicLR(\n",
    "      mode='triangular',\n",
    "      base_lr=1e-5, \n",
    "      max_lr=1e-4,\n",
    "      step_size= step_size\n",
    "      )\n",
    "  \n",
    "  model = build_model()\n",
    "\n",
    "  epochs = 100\n",
    "  # Train\n",
    "  history = model.fit(train_gen,\n",
    "                          epochs=epochs,  validation_data = valid_gen,\n",
    "                          callbacks = [tfk.callbacks.EarlyStopping(monitor= 'val_accuracy', mode='max', patience=10, restore_best_weights=True), clr], workers =8 , use_multiprocessing = True\n",
    "                          )\n",
    "  \n",
    "  val_acc_per_fold.append(history.history[\"val_accuracy\"] * 100)\n",
    "  val_loss_per_fold.append(history.history[\"val_loss\"])\n",
    "  acc_per_fold.append(history.history[\"accuracy\"] * 100)\n",
    "  loss_per_fold.append(history.history[\"loss\"])\n",
    "\n",
    "  # Plot the training\n",
    "  plt.figure(figsize=(20,5))\n",
    "  plt.plot(history.history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "  plt.plot(history.history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "  plt.legend(loc='upper left')\n",
    "  plt.title('Category Crossentropy')\n",
    "  plt.grid(alpha=.3)\n",
    "\n",
    "  plt.figure(figsize=(20,5))\n",
    "  plt.plot(history.history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "  plt.plot(history.history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "  plt.legend(loc='upper left')\n",
    "  plt.title('Accuracy')\n",
    "  plt.grid(alpha=.3)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "  #prediction on test set\n",
    "  predictions = model.predict(test_gen_ovaN)\n",
    "\n",
    "\n",
    "  y = list(test_ovaN.label)[:2368]\n",
    "  pred = (predictions > 0.5).astype(int)\n",
    "  target_names = ['O', 'N']\n",
    "  cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "  # Compute the classification metrics\n",
    "  accuracy = accuracy_score(y, pred)\n",
    "  precision = precision_score(y, pred, average='macro')\n",
    "  recall = recall_score(y, pred, average='macro')\n",
    "  f1 = f1_score(y, pred, average='macro')\n",
    "  print('Accuracy:',accuracy.round(4))\n",
    "  print('Precision:',precision.round(4))\n",
    "  print('Recall:',recall.round(4))\n",
    "  print('F1:',f1.round(4))\n",
    "  print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "  # Plot the confusion matrix\n",
    "  plt.figure(figsize=(10,10))\n",
    "  sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "  plt.xlabel('True labels')\n",
    "  plt.ylabel('Predicted labels')\n",
    "  plt.show()\n",
    "\n",
    "  # Model saving\n",
    "  model_directory = './'\n",
    "  filename = 'cnn_N_kFold_' + str(i+1)\n",
    "  filename_chosen = os.path.join(model_directory, filename)\n",
    "  model.save(filename_chosen + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CSHfIAO4y8l"
   },
   "source": [
    "### Load trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNrb1QyZLnZK"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# load models \n",
    "total_model = 5\n",
    "model_directory = './'\n",
    "trained_models = list()\n",
    "for model_n in range(total_model):\n",
    "  filename = 'cnn_N_kFold_' + str(model_n+1)\n",
    "  filename_chosen = os.path.join(model_directory, filename)\n",
    "  tl_model = load_model(filename_chosen + '.h5')\n",
    "    # Compile the model\n",
    "  tl_model.compile(loss=tfk.losses.BinaryFocalCrossentropy(True), optimizer=tfk.optimizers.Adam(learning_rate = 1e-4), metrics='accuracy')\n",
    "  trained_models.append(tl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSQrS3nR4387"
   },
   "source": [
    "### Evaluate trained models (for storing statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAP0uKqZ42eZ"
   },
   "outputs": [],
   "source": [
    "validation_stats = {}\n",
    "plt.get_current_fig_manager().full_screen_toggle() # toggle fullscreen mode\n",
    "\n",
    "for idx, svm_model in enumerate(trained_models):  \n",
    "  predictions = svm_model.predict(test_gen)\n",
    "  y = np.argmax(tfk.utils.to_categorical(list(test.label))[:-2], axis = -1)\n",
    "  pred = np.argmax(predictions, axis=-1)\n",
    "  target_names = ['N', 'P', 'T']\n",
    "  cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "  # Compute the classification metrics\n",
    "  accuracy = accuracy_score(y, pred)\n",
    "  precision = precision_score(y, pred, average='macro')\n",
    "  recall = recall_score(y, pred, average='macro')\n",
    "  f1 = f1_score(y, pred, average='macro')\n",
    "\n",
    "\n",
    "  validation_stats['fold_' + str(idx+1) ] = {\n",
    "      'accuracy' : accuracy,\n",
    "      'precision' :precision,\n",
    "      'recall' :recall,\n",
    "      'f1' :f1,\n",
    "      'classification_report' :classification_report(y, pred, target_names=target_names, digits=4, output_dict = True)}\n",
    "\n",
    "\n",
    "  print('Accuracy:',accuracy.round(4))\n",
    "  print('Precision:',precision.round(4))\n",
    "  print('Recall:',recall.round(4))\n",
    "  print('F1:',f1.round(4))\n",
    "  print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "  # Plot the confusion matrix\n",
    "  plt.figure(figsize=(10,10))\n",
    "  hm = sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "  plt.xlabel('True labels')\n",
    "  plt.ylabel('Predicted labels')\n",
    "  plt.show()\n",
    "\n",
    "  hm.get_figure().savefig(\"alice/data_for_report/heatmap_fold_\"+str(idx+1)+\".pdf\")\n",
    "\n",
    "with open('alice/data_for_report/test_stats.json' , 'w') as fp:\n",
    "    json.dump(validation_stats, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hle5LNGKJox"
   },
   "source": [
    "## Second binary classifier: T and focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zX8GeIxKJox"
   },
   "outputs": [],
   "source": [
    "supernet2 = tf.keras.applications.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(480,480,3)\n",
    ")\n",
    "\n",
    "\n",
    "supernet2._name = \"effnet2\"\n",
    "\n",
    "\n",
    "\n",
    "count = 1\n",
    "print(len(supernet2.layers))\n",
    "for layer in supernet2.layers:\n",
    "    if count < 80:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZuhshFTKJox"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D, MaxPooling2D, BatchNormalization,Concatenate, Resizing\n",
    "from keras import regularizers\n",
    "\n",
    "inputs = tfk.Input((480,480,3))\n",
    "\n",
    "y2 = supernet2(inputs)\n",
    "\n",
    "y2 = tf.keras.layers.GlobalAveragePooling2D()(y2)\n",
    "y2 = tf.keras.layers.Dense(256, activation='relu', kernel_initializer = tfk.initializers.HeUniform(SEED))(y2)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer = tfk.initializers.GlorotUniform(SEED))(y2)\n",
    "\n",
    "# Connect input and output through the Model class\n",
    "tl_model_exp3 = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model\n",
    "tl_model_exp3.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(True), optimizer=tfk.optimizers.experimental.AdamW(), metrics='accuracy')\n",
    "\n",
    "tl_model_exp3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65yDlP5HKJoy"
   },
   "outputs": [],
   "source": [
    "tl_model_exp3.load_weights(\"/gdrive/MyDrive/tuberculosis-pneumonia-classification/model_exp3_N2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77vTTyb5KJoy"
   },
   "outputs": [],
   "source": [
    "training_samples = int(len(train_gen_ovaT)*batch_size)\n",
    "step_size = 6*training_samples // batch_size\n",
    "\n",
    "clr = CyclicLR(\n",
    "    mode='triangular',\n",
    "    base_lr=1e-5, \n",
    "    max_lr=1e-4,             \n",
    "    step_size= step_size\n",
    "    )\n",
    "\n",
    "history = tl_model_exp3.fit(\n",
    "    train_gen_ovaT,\n",
    "    workers=8,\n",
    "    use_multiprocessing=True,\n",
    "    epochs = 35,\n",
    "    validation_data = valid_gen_ovaT,\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor= 'val_accuracy', mode='max', patience=10, restore_best_weights=True), clr],\n",
    ").history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZn1JUkPKJoy"
   },
   "source": [
    "### Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOBFgsVmKJoy"
   },
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Category Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4470DBhKJoy"
   },
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_-2JBBrKJoz"
   },
   "outputs": [],
   "source": [
    "# Predict the test set with the CNN\n",
    "predictions = tl_model_exp3.predict(test_gen_ovaT,\n",
    "                                    workers=8,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JDuZQuBKJoz"
   },
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NAVT_JEKJoz"
   },
   "outputs": [],
   "source": [
    "y = list(test_ovaT.label)[:2368]\n",
    "pred = (predictions > 0.5).astype(int)\n",
    "target_names = ['O', 'T']\n",
    "cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(y, pred)\n",
    "precision = precision_score(y, pred, average='macro')\n",
    "recall = recall_score(y, pred, average='macro')\n",
    "f1 = f1_score(y, pred, average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agfEH1RqKJo0"
   },
   "outputs": [],
   "source": [
    "tl_model_exp3.save(\"model_exp3_T2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ELTlntyKJo0"
   },
   "source": [
    "## Third binary classifier: P and BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndRIbCm1KJo0"
   },
   "outputs": [],
   "source": [
    "supernet2 = tf.keras.applications.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "supernet2._name = \"effnet2\"\n",
    "\n",
    "\n",
    "\n",
    "count = 1\n",
    "print(len(supernet2.layers))\n",
    "for layer in supernet2.layers:\n",
    "    if count < 80:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEysNYwiKJo0"
   },
   "outputs": [],
   "source": [
    "# supernet.trainable = False\n",
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D, MaxPooling2D, BatchNormalization,Concatenate, Resizing\n",
    "from keras import regularizers\n",
    "\n",
    "inputs = tfk.Input((224,224,3))\n",
    "\n",
    "y2 = supernet2(inputs)\n",
    "\n",
    "y2 = tf.keras.layers.GlobalAveragePooling2D()(y2)\n",
    "y2 = tf.keras.layers.Dense(256, activation='relu', kernel_initializer = tfk.initializers.HeUniform(SEED))(y2)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer = tfk.initializers.GlorotUniform(SEED))(y2)\n",
    "\n",
    "# Connect input and output through the Model class\n",
    "tl_model_exp3 = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model\n",
    "tl_model_exp3.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(True), optimizer=tfk.optimizers.Adam(learning_rate = 1e-4), metrics='accuracy')\n",
    "\n",
    "tl_model_exp3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2sixtTJKJo0"
   },
   "outputs": [],
   "source": [
    "tl_model_exp3.load_weights(\"/gdrive/MyDrive/tuberculosis-pneumonia-classification/model_exp3_N2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_EuaiJeKJo1"
   },
   "outputs": [],
   "source": [
    "training_samples = int(len(train_gen_ovaN)*batch_size)\n",
    "step_size = 6*training_samples // batch_size\n",
    "\n",
    "clr = CyclicLR(\n",
    "    mode='triangular',\n",
    "    base_lr=1e-5, \n",
    "    max_lr=1e-4,             \n",
    "    step_size= step_size\n",
    "    )\n",
    "\n",
    "history = tl_model_exp3.fit(\n",
    "    train_gen_ovaP,\n",
    "    workers=8,\n",
    "    use_multiprocessing=True,\n",
    "    epochs = 35,\n",
    "    validation_data = valid_gen_ovaP,\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor= 'val_accuracy', mode='max', patience=5, restore_best_weights=True), clr],\n",
    ").history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6edd76jVKJo1"
   },
   "source": [
    "### Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaM-VH25KJo1"
   },
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Category Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HproP21OKJo1"
   },
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbsw67yRKJo1"
   },
   "outputs": [],
   "source": [
    "# Predict the test set with the CNN\n",
    "predictions = tl_model_exp3.predict(test_gen_ovaP,\n",
    "                                    workers=8,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGd3gRVYKJo2"
   },
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8saFJ0gkKJo2"
   },
   "outputs": [],
   "source": [
    "y = list(test_ovaP.label)[:2368]\n",
    "pred = (predictions > 0.5).astype(int)\n",
    "target_names = ['O', '  P']\n",
    "cm = confusion_matrix(y, pred, normalize=\"true\")\n",
    "\n",
    "\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(y, pred)\n",
    "precision = precision_score(y, pred, average='macro')\n",
    "recall = recall_score(y, pred, average='macro')\n",
    "f1 = f1_score(y, pred, average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "print(classification_report(y, pred, target_names=target_names, digits=4))\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm.T, xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntNPJfhuKJo2"
   },
   "outputs": [],
   "source": [
    "tl_model_exp3.save(\"model_exp3_P2.h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "gpuClass": "premium"
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
