{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#IMPORTS"
   ],
   "metadata": {
    "id": "DcnLzzMAHm_n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tensorflow==2.10.1  -q gwpy"
   ],
   "metadata": {
    "id": "kSvgWEsNF48w",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682331153459,
     "user_tz": -120,
     "elapsed": 74960,
     "user": {
      "displayName": "Elisa Fumagalli",
      "userId": "01300940458680453558"
     }
    },
    "outputId": "a72633b5-b26e-451a-afd6-b669a4b63d59"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m578.1/578.1 MB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m42.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m56.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m62.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.9/5.9 MB\u001B[0m \u001B[31m68.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m438.7/438.7 kB\u001B[0m \u001B[31m31.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.6/42.6 kB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.4/45.4 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m51.0/51.0 kB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.9/4.9 MB\u001B[0m \u001B[31m102.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Building wheel for ligo-segments (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0m"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip uninstall matplotlib\n",
    "!pip install matplotlib==3.1.3"
   ],
   "metadata": {
    "id": "ZMu-HP0vF6wo",
    "outputId": "0516cd6d-851b-420c-d959-2d977161f90c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: matplotlib 3.7.1\n",
      "Uninstalling matplotlib-3.7.1:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.9/dist-packages/matplotlib-3.7.1-py3.9-nspkg.pth\n",
      "    /usr/local/lib/python3.9/dist-packages/matplotlib-3.7.1.dist-info/*\n",
      "    /usr/local/lib/python3.9/dist-packages/matplotlib/*\n",
      "    /usr/local/lib/python3.9/dist-packages/mpl_toolkits/axes_grid1/*\n",
      "    /usr/local/lib/python3.9/dist-packages/mpl_toolkits/axisartist/*\n",
      "    /usr/local/lib/python3.9/dist-packages/mpl_toolkits/mplot3d/*\n",
      "    /usr/local/lib/python3.9/dist-packages/pylab.py\n",
      "Proceed (Y/n)? "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "from google.colab import drive\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import albumentations as A\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array,array_to_img\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.callbacks import *\n",
    "import random\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "from skimage.measure import label as label_fn\n",
    "import cv2\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(tf.__version__)\n",
    "import albumentations as A\n",
    "from itertools import combinations\n",
    "import json\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from skimage import filters\n",
    "import scipy"
   ],
   "metadata": {
    "id": "AY8uEtycF-O5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install lime"
   ],
   "metadata": {
    "id": "FXYEFM3JLEHv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import lime\n",
    "from lime import lime_image\n",
    "from lime import submodular_pick\n",
    "from skimage.segmentation import mark_boundaries"
   ],
   "metadata": {
    "id": "Qx5Kjh4MGKk7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#MOUNT DRIVE"
   ],
   "metadata": {
    "id": "eB0bsAS1Hy09"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "HdhHuulyGBMX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/drive/MyDrive/AAIB\n",
    "PATH = '/content/drive/MyDrive/tuberculosis-pneumonia-classification'"
   ],
   "metadata": {
    "id": "OuXrqg3hG5S8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "SEED = 4224\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "labels_path = 'data/labels_train_clean.csv'\n",
    "all_data_no_duplicates_path = 'data/train_all_no_duplicates'\n",
    "clean_data_path = 'data/train_clean/'\n",
    "noisy_data_path = 'data/train_noisy/'\n",
    "\n",
    "train_percentage = 0.8\n",
    "validation_percentage = 0.15\n",
    "test_percentage = 0.2\n",
    "img_size = (224,224)\n",
    "batch_size = 16\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "sns.despine()\n",
    "sns.set_context('paper')"
   ],
   "metadata": {
    "id": "KcfbuwFbG7Zz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#DATA GENERATOR"
   ],
   "metadata": {
    "id": "jUWnX6B1H3oS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomGenerator(tf.keras.utils.Sequence):\n",
    "  \"\"\"\n",
    "    CustomGenerator inheriting from tf.keras.utils.Sequence.\n",
    "\n",
    "    We have to implement 3 main methods:\n",
    "      - __init__: save dataset params like directory, filenames, etc.\n",
    "      - __len__: return the total number of samples in the dataset (number of batches)\n",
    "      - __getitem__: return a single batch of paired images masks\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, \n",
    "               dataframe, # dataframe of the dataset  \n",
    "               base_path,\n",
    "               preprocessing_function=None, # Preprocessing function (e.g., the one used for transfer learning)\n",
    "               batch_size=16, # Batch size\n",
    "               out_shape = (100,100),\n",
    "               shuffle=False,\n",
    "               categorical = True,\n",
    "               augment = False,\n",
    "               seed = SEED,\n",
    "               flow_from_directory = True,\n",
    "               preprocess_input = False):\n",
    "    \n",
    "    # Get all filenames\n",
    "    if isinstance(base_path, Tuple):\n",
    "      self.filenames = []\n",
    "      for p in base_path:\n",
    "\n",
    "        paths = self.folderToPaths(p, full_path = False)\n",
    "\n",
    "        for pa in paths:\n",
    "          if pa in set(dataframe.file):\n",
    "            self.filenames.append(os.path.join(p, pa))\n",
    "\n",
    "\n",
    "    else:\n",
    "        self.filenames = [os.path.join(base_path, img_path) for img_path in list(dataframe.file)]\n",
    "\n",
    "    self.labels = tfk.utils.to_categorical(list(dataframe.label)) if categorical else list(dataframe.label)\n",
    "\n",
    "    # Set indices list in [0, len(subset_filenames)]\n",
    "    self.indices = np.arange(len(self.filenames))\n",
    "\n",
    "    # Save dataset parameters as class attributes\n",
    "    self.base_path = base_path\n",
    "    self.preprocessing_function = preprocessing_function\n",
    "    self.out_shape = out_shape\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = shuffle\n",
    "    self.augment = augment\n",
    "    self.seed = seed\n",
    "    self.flow_from_directory =flow_from_directory\n",
    "    self.data_augmentation = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit = 0.05, contrast_limit=0.05, p=0.5),\n",
    "    A.ShiftScaleRotate(p = 0.8, rotate_limit = 20, scale_limit = 0.3, border_mode =  cv2.BORDER_CONSTANT, value = 0),\n",
    "    A.CLAHE(p=0.2)\n",
    "    ])\n",
    "    self.preprocess_input = preprocess_input\n",
    "\n",
    "    if not self.flow_from_directory:\n",
    "      self.images = self.load_all_imgs()\n",
    "\n",
    "  def augmentation(self, images):\n",
    "    return self.data_augmentation(image = images)\n",
    "\n",
    "\n",
    "  def __filterNoisyOnClahe(self, image):\n",
    "    clahe = cv2.createCLAHE(clipLimit = 300, tileGridSize = (50, 50))\n",
    "    im1 = cv2.resize(image, (400, 400))\n",
    "    im1 = scipy.ndimage.gaussian_laplace(im1, sigma = 6)\n",
    "    im1 = clahe.apply(im1)\n",
    "    var1 = np.var(im1)\n",
    "    if var1 > 800:\n",
    "      image= cv2.medianBlur(image, ksize=5)\n",
    "      return scipy.ndimage.uniform_filter(image, size=3)\n",
    "    else:\n",
    "      return image\n",
    "\n",
    "\n",
    "  def __sharpenImage(self, image):\n",
    "    sharpen_kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "\n",
    "    sharpened = cv2.filter2D(image, -1, sharpen_kernel)\n",
    "\n",
    "    return sharpened\n",
    "\n",
    "  def __invert_image(self, img):\n",
    "\n",
    "    otsu_thresh = filters.threshold_otsu(img)\n",
    "    masked_image = (img > otsu_thresh) * 1.0\n",
    "    valsROI1, _ = np.histogram(masked_image[220:244, 220:244], bins=2, range=(0, 1))\n",
    "    valsROI2, _ = np.histogram(masked_image[0:25, 0:25], bins=2, range=(0, 1))\n",
    "    valsROI3, _ = np.histogram(masked_image[0:25, 220:244], bins=2, range=(0, 1))\n",
    "    valsROI4, _ = np.histogram(masked_image[220:244, 0:25], bins=2, range=(0, 1))\n",
    "    \n",
    "    valsTot = valsROI1 + valsROI2 + valsROI3 + valsROI4\n",
    "    \n",
    "    labels = label_fn(masked_image)\n",
    "\n",
    "    if len(np.unique(labels)) < 100:\n",
    "        if valsTot[0] > valsTot[1]:\n",
    "            return img\n",
    "        else:\n",
    "            return 255 - img\n",
    "    return img\n",
    "\n",
    "  def __filterBlurred(self, image):\n",
    "    clahe = cv2.createCLAHE(clipLimit = 1.8, tileGridSize = (4, 4))\n",
    "    minThresh = 2\n",
    "    im1 = cv2.resize(image, (400, 400))\n",
    "    im1 = scipy.ndimage.gaussian_laplace(im1, sigma = 2)\n",
    "    im1 = clahe.apply(im1)\n",
    "    _, im1 = cv2.threshold(im1, minThresh, 255, cv2.THRESH_BINARY)\n",
    "    var1 = np.var(im1)\n",
    "    if var1 < 5:\n",
    "        return self.__sharpenImage(image)\n",
    "    else:\n",
    "        return self.__sharpenImage(image)\n",
    "\n",
    "  def __filterUnderexposed(self, image):\n",
    "    im1 = cv2.resize(image, (400, 400))\n",
    "    mean1 = np.mean(im1)\n",
    "    if mean1 < 71:\n",
    "      clahe = cv2.createCLAHE(clipLimit = 2, tileGridSize = (2, 2))\n",
    "      image = clahe.apply(image)\n",
    "      return image\n",
    "    else:\n",
    "      return image\n",
    "\n",
    "\n",
    "  def preprocess(self, image):\n",
    "\n",
    "    image = self.__invert_image(image)\n",
    "    image = self.__filterUnderexposed(image)\n",
    "    image = self.__filterNoisyOnClahe(image)\n",
    "    image = self.__filterBlurred(image)\n",
    "   \n",
    "\n",
    "    return image\n",
    "\n",
    "  def __len__(self):\n",
    "    # Return the length of the dataset (number of batches)\n",
    "    # that is given by #images // batch_size\n",
    "    return len(self.filenames) // self.batch_size\n",
    "\n",
    "  def on_epoch_start(self):\n",
    "    # Shuffle indices after each epoch\n",
    "    if self.shuffle == True:\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "  def load_all_imgs(self):\n",
    "      images = []\n",
    "      for f in self.filenames:\n",
    "        image = cv2.imread(f, 0)\n",
    "        image = cv2.resize(image, (self.out_shape))\n",
    "        if self.preprocess_input:\n",
    "          image = self.preprocess(image)\n",
    "        images.append(image)\n",
    "\n",
    "      return np.array(images)\n",
    "\n",
    "  def get_image_and_label(self, index):\n",
    "\n",
    "    if not self.flow_from_directory:\n",
    "      image = self.images[index]\n",
    "      if self.augment:\n",
    "        image = self.augmentation(image)\n",
    "      image = np.squeeze(image)\n",
    "      image = np.stack([image, image, image], axis = -1)\n",
    "      curr_label = self.labels[index]\n",
    "    else:\n",
    "      curr_filename = self.filenames[index] # Get filename at index\n",
    "      curr_label = self.labels[index]\n",
    "      image = cv2.imread(curr_filename, 0)\n",
    "      image = cv2.resize(image, (self.out_shape))\n",
    "      if self.preprocess_input:\n",
    "        image = self.preprocess(image)\n",
    "\n",
    "      if self.augment:\n",
    "        image = self.augmentation(image)['image']\n",
    "\n",
    "      image = np.stack([image, image, image], axis = -1)\n",
    "\n",
    "\n",
    "    return image, curr_label\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # In this function we generate a batch (of size self.batch_size) of images and corresponding masks\n",
    "    \n",
    "    # Get 'self.batch_size' indices\n",
    "    current_indices = self.indices[index*self.batch_size:(index*self.batch_size)+self.batch_size]\n",
    "\n",
    "    \"\"\"if len(current_indices) == 0:\n",
    "      current_indices = self.indices[len(self.indices)-self.batch_size:len(self.indices)]\"\"\"\n",
    "\n",
    "    # Init lists that will contain images and masks\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "\n",
    "    # Cycle over the indices\n",
    "    for idx in current_indices:\n",
    "      # Get single image/mask at index 'idx'\n",
    "      image, label = self.get_image_and_label(idx)\n",
    "\n",
    "      # Apply the preprocessing function\n",
    "      if self.preprocessing_function is not None:\n",
    "        image = self.preprocessing_function(image)\n",
    "\n",
    "      # Append both image and mask (with added batch dimension) to the corresponding batch lists\n",
    "      batch_images.append(np.expand_dims(image, 0))\n",
    "      batch_labels.append(label)\n",
    "     \n",
    "    # Finally, obtain a final batch by concatenating all the images over the batch dimension\n",
    "    batch_images = np.concatenate(batch_images, axis=0)\n",
    "    batch_labels = np.array(batch_labels)\n",
    "\n",
    "    return batch_images, batch_labels\n",
    "\n",
    "\n",
    "  def folderToPaths(\n",
    "        self,\n",
    "        full_img_dir,\n",
    "        full_path = True\n",
    "):\n",
    "\n",
    "    x_paths_list = []\n",
    "\n",
    "    full_img_dir = full_img_dir\n",
    "\n",
    "    for full in os.listdir(full_img_dir):\n",
    "         if full_path:\n",
    "            x_paths_list.append(os.path.join(full_img_dir, full))\n",
    "         else:\n",
    "          x_paths_list.append(full)\n",
    "    \n",
    "    x_paths_list.sort()\n",
    "    return x_paths_list"
   ],
   "metadata": {
    "id": "ZQzL2OaIHB_w"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#DATA LOADING"
   ],
   "metadata": {
    "id": "nv494HJAICNM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def encode(x):\n",
    "  if x == 'N':\n",
    "    return 0\n",
    "  elif x == 'P':\n",
    "    return 1\n",
    "  else:\n",
    "    return 2"
   ],
   "metadata": {
    "id": "Al_1t9WrHF26"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def folderToPaths(\n",
    "        full_img_dir,\n",
    "):\n",
    "\n",
    "    x_paths_list = []\n",
    "\n",
    "    full_img_dir = full_img_dir\n",
    "\n",
    "    for full in os.listdir(full_img_dir):\n",
    "         x_paths_list.append(os.path.join(full_img_dir, full))\n",
    "    \n",
    "    x_paths_list.sort()\n",
    "    return x_paths_list"
   ],
   "metadata": {
    "id": "vfsq2YV0HIJJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labelsDF = pd.read_csv(labels_path)\n",
    "display(labelsDF.head(20))"
   ],
   "metadata": {
    "id": "WfLkpe4oHKRe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labelsDF.label = labelsDF.label.apply(lambda x: encode(x))\n",
    "display(labelsDF.head(20))"
   ],
   "metadata": {
    "id": "C9XUrFYDHMxQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "all_data_no_duplicates_path_list = folderToPaths(full_img_dir = all_data_no_duplicates_path)"
   ],
   "metadata": {
    "id": "fM_z_pwnHQZ8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_val, test = train_test_split(labelsDF, test_size = test_percentage, shuffle = True, stratify = labelsDF.label, random_state=SEED)"
   ],
   "metadata": {
    "id": "cacwICaUHTDf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_gen = CustomGenerator(dataframe = test, base_path = 'data/train_all_no_duplicates', batch_size = batch_size, out_shape = img_size, shuffle = True, flow_from_directory=True, preprocess_input = True, categorical = True, augment = False)"
   ],
   "metadata": {
    "id": "d3Z5HqdlHZCc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#MODEL LOADING"
   ],
   "metadata": {
    "id": "dZbE6q0DIFOR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define metrics\n",
    "METRICS = [tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "] \n",
    "\n",
    "svm_model_loaded = tf.keras.models.load_model(\"svm_all_data.h5\")\n",
    "svm_model_loaded.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  metrics=METRICS)"
   ],
   "metadata": {
    "id": "A1Jsi4zqGfqk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#LIME EXPLAINABILITY"
   ],
   "metadata": {
    "id": "vkHhOxNOIICD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "explainer = lime_image.LimeImageExplainer()"
   ],
   "metadata": {
    "id": "GOxF-2S9GMzS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def getSVMProbabilities(img):\n",
    "  predictions = svm_model_loaded.predict(img) # one-vs-all scores\n",
    "  predictions = np.exp(predictions)/np.sum(np.exp(predictions),axis=1, keepdims=True) # softmax after the scoring\n",
    "  return predictions"
   ],
   "metadata": {
    "id": "hEcD75fnGNet"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#NORMAL"
   ],
   "metadata": {
    "id": "XbFsw_ET9WIF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "iterator = iter(test_gen)\n",
    "images, labels = next(iterator)"
   ],
   "metadata": {
    "id": "sZ5wUBi1Gp_-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "temp, mask = exp.get_image_and_mask(exp.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
    "\n",
    "plt.imshow(mark_boundaries(temp, mask, color=(1, 0, 0)))"
   ],
   "metadata": {
    "id": "l_W6pk0WGugi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def show_xai_results(image, path):\n",
    "  exp = explainer.explain_instance(image, \n",
    "                                 getSVMProbabilities, \n",
    "                                 top_labels=3, \n",
    "                                 hide_color=0, \n",
    "                                 num_samples=1000,\n",
    "                                 random_seed = SEED)\n",
    "  temp, mask = exp.get_image_and_mask(exp.top_labels[0], positive_only=False, num_features=5, hide_rest=False)\n",
    "\n",
    "  #Select the same class explained on the figures above.\n",
    "  ind =  exp.top_labels[0]\n",
    "\n",
    "  #Map each explanation weight to the corresponding superpixel\n",
    "  dict_heatmap = dict(exp.local_exp[ind])\n",
    "  heatmap = np.vectorize(dict_heatmap.get)(exp.segments) \n",
    "\n",
    "  fig, axis = plt.subplots(1,3, figsize = (10,30))\n",
    "  axis = axis.flatten()\n",
    "\n",
    "  axis[0].imshow(image, cmap='gray')\n",
    "  axis[0].set_axis_off()\n",
    "  \n",
    "\n",
    "  axis[1].imshow(mark_boundaries(temp, mask, color=(1, 0, 0)))\n",
    "  axis[1].set_axis_off()\n",
    "\n",
    "  aim = axis[2].imshow(heatmap, cmap = 'RdBu', vmin  = -heatmap.max(), vmax = heatmap.max())\n",
    "  axis[2].set_axis_off()\n",
    "\n",
    "  fig.colorbar(aim, ax=axis[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "  plt.get_current_fig_manager().full_screen_toggle()\n",
    "  plt.savefig(path, bbox_inches='tight')\n",
    "  plt.show()\n",
    "  "
   ],
   "metadata": {
    "id": "YeqK9ATMYlse"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "show_xai_results(images[0], path = 'data_for_report/svm-xai-normal-exp.pdf')"
   ],
   "metadata": {
    "id": "phJvlwtJaCL0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#PNEUMONIA"
   ],
   "metadata": {
    "id": "FqwJAtNz9f2h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "show_xai_results(images[6], path = 'data_for_report/svm-xai-pneumonia-hm.pdf')"
   ],
   "metadata": {
    "id": "62wHZ4MCf3TX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Tuberculusis"
   ],
   "metadata": {
    "id": "YC-_Bdli9uiJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "show_xai_results(images[9], path = 'data_for_report/svm-xai-tuberculosis-hm.pdf')"
   ],
   "metadata": {
    "id": "bet04Ow_gFmt"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
